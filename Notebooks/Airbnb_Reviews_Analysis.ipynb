{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59e3b346-86a2-4e36-8b36-adcc16e4e7c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Airbnb Reviews Pipeline - MiniLM + SST-2\n",
    "\n",
    "**Goal:** Extract negative, issue-tagged sentences from Airbnb reviews and produce actionable aggregations (property + host).\n",
    "\n",
    "## Pipeline stages\n",
    "1) Load + filter Italy\n",
    "2) Split reviews blob → review chunks (review-level)\n",
    "3) Split review chunks → sentences (English-only, min_words, smart dedup)\n",
    "4) Top-K per property prefilter (reduce volume before embeddings)\n",
    "5) Similarity (MiniLM) on candidate unique sentences and Apply similarity thresholds → issue candidates\n",
    "7) Sentiment (SST-2) on issue-candidates (sharded + resume)\n",
    "8) Join (issue × negative) → final issues (unique + occurrences)\n",
    "9) Aggregations + evidence tables\n",
    "10) LLM recommendations API on selected properties.\n",
    "\n",
    "## Locked decisions\n",
    "- Sentence filters: English-only (ASCII), `min_words=5`, smart normalization + dedup by `sentence_norm`\n",
    "- Similarity: MiniLM computed once (no second pass)\n",
    "- Similarity thresholds: `ISSUE_SIM_THRESHOLD=0.4`, `TOP2_GAP_MAX=0.04` (Top2 only if very close)\n",
    "- Sentiment: SST-2, keep if `neg_prob >= 0.40`\n",
    "- Outputs: final issues tables + LLM recommendations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40e570f-3632-43d9-bd97-95ee097db325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"replace_by_storage_account\"  \n",
    "container = \"replace_by_container\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dcdb602-6fed-406e-b53d-30672bcbcc55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sas_token=\"replace_with_your_sas_token\"\n",
    "sas_token = sas_token.lstrip('?')\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d582cb-5aef-4fdb-a424-58a02e5a64b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/airbnb_1_12_parquet\"\n",
    "\n",
    "airbnb = spark.read.parquet(path)\n",
    "display(airbnb.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3be9400f-22dd-48f6-af0f-98adc107a9b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = airbnb.withColumn(\"country_from_location\",\n",
    "                       F.trim(F.element_at(F.split(F.col(\"location\"), \",\"), -1)))\n",
    "\n",
    "df_italy = df.filter(F.upper(F.col(\"country_from_location\")) == \"ITALY\")\n",
    "\n",
    "print(\"[Italy] rows:\", df_italy.count())\n",
    "print(\"[Italy] unique properties:\", df_italy.select(\"property_id\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d93db266-269c-42ac-a4d1-f2fea88aac8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, FloatType, BooleanType\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "OUTPUT_BASE = \"dbfs:/FileStore/airbnb\"\n",
    "RUN_ID = \"ITALY_1\"\n",
    "print(f\"[INFO] RUN_ID={RUN_ID}\")\n",
    "\n",
    "def save_table_and_path(df, table_name: str, mode: str = \"overwrite\"):\n",
    "    # keep as metastore table (for spark.table usage)\n",
    "    df.write.mode(mode).format(\"delta\").saveAsTable(table_name)\n",
    "\n",
    "    # save a Delta copy to DBFS path\n",
    "    out_path = f\"{OUTPUT_BASE}/{table_name}\"\n",
    "    df.write.mode(mode).format(\"delta\").save(out_path)\n",
    "    print(f\"[OK] Saved table={table_name} | path={out_path}\")\n",
    "\n",
    "# Column names\n",
    "PROPERTY_COL   = \"property_id\"\n",
    "SELLER_INFO_COL = \"seller_info\"\n",
    "SELLER_ID_COL  = \"seller_id\"\n",
    "REVIEWS_COL    = \"reviews\"  \n",
    "\n",
    "REVIEW_ID_COL_CANDIDATES = [\"review_id\", \"id\", \"reviewId\"]\n",
    "\n",
    "# Thresholds (locked) \n",
    "MIN_WORDS = 5\n",
    "\n",
    "ISSUE_SIM_THRESHOLD = 0.4\n",
    "TOP_GAP_MIN = 0.05\n",
    "TOP2_GAP_MAX = 0.04\n",
    "\n",
    "NEG_PROB_THRESHOLD = 0.40\n",
    "\n",
    "# Models\n",
    "SIM_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "SENT_MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Output tables\n",
    "T_REVIEWS_BASE          = f\"airbnb_reviews_base_{RUN_ID}\"\n",
    "\n",
    "T_SENTENCES_OCCUR       = f\"airbnb_sentences_occurrences_{RUN_ID}\"\n",
    "T_SENTENCES_UNIQUE      = f\"airbnb_sentences_unique_{RUN_ID}\"\n",
    "\n",
    "T_SIM_UNIQUE            = f\"airbnb_similarity_unique_{RUN_ID}\"\n",
    "\n",
    "T_SENTIMENT_UNIQUE      = f\"airbnb_sentiment_unique_{RUN_ID}\"\n",
    "T_NEG_UNIQUE            = f\"airbnb_negative_unique_{RUN_ID}\"\n",
    "T_NEG_OCCUR             = f\"airbnb_negative_occurrences_{RUN_ID}\"\n",
    "\n",
    "T_ISSUES_UNIQUE          = f\"airbnb_issues_unique_{RUN_ID}\"\n",
    "T_ISSUES_OCCUR           = f\"airbnb_issues_occurrences_{RUN_ID}\"\n",
    "\n",
    "T_PROP_ISSUE_AGG         = f\"airbnb_property_issue_agg_{RUN_ID}\"\n",
    "T_HOST_ISSUE_AGG         = f\"airbnb_host_issue_agg_{RUN_ID}\"\n",
    "\n",
    "T_PROP_ISSUE_EVID        = f\"airbnb_property_issue_evidence_{RUN_ID}\"\n",
    "T_HOST_ISSUE_EVID        = f\"airbnb_host_issue_evidence_{RUN_ID}\"\n",
    "\n",
    "T_ISSUES_CANDIDATES_UNIQUE = f\"airbnb_issues_candidates_unique_{RUN_ID}\"\n",
    "T_ISSUES_CANDIDATES_OCCUR  = f\"airbnb_issues_candidates_occurrences_{RUN_ID}\"\n",
    "\n",
    "T_SENTENCES_CANDIDATES_OCCUR  = f\"airbnb_sentences_candidates_occurrences_{RUN_ID}\"\n",
    "T_SENTENCES_CANDIDATES_UNIQUE = f\"airbnb_sentences_candidates_unique_{RUN_ID}\"\n",
    "\n",
    "T_SENT_PROGRESS = f\"airbnb_sent_progress_{RUN_ID}\"\n",
    "T_SENT_INPUT    = f\"airbnb_sent_input_{RUN_ID}\"\n",
    "\n",
    "print(\"[INFO] Output tables:\")\n",
    "for t in [\n",
    "    T_REVIEWS_BASE, T_SENTENCES_OCCUR, T_SENTENCES_UNIQUE, T_SIM_UNIQUE,\n",
    "    T_SENTIMENT_UNIQUE, T_NEG_UNIQUE, T_NEG_OCCUR, T_ISSUES_UNIQUE, T_ISSUES_OCCUR,\n",
    "    T_PROP_ISSUE_AGG, T_HOST_ISSUE_AGG, T_PROP_ISSUE_EVID, T_HOST_ISSUE_EVID\n",
    "]:\n",
    "    print(\" -\", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9ec32d6-75ad-4253-a151-de4dd51a8a21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Issues taxonomy (final approved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "267e605b-1db9-469d-bce7-19fa86c0c30d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ISSUE_TEXTS = {\n",
    "    \"wifi\": (\n",
    "        \"internet connection quality, wifi speed, weak or unstable signal, \"\n",
    "        \"slow internet, connection drops, unreliable wifi for work or streaming\"\n",
    "    ),\n",
    "    \"tv_and_entertainment\": (\n",
    "        \"television, TV functionality, smart TV, streaming services, Netflix, \"\n",
    "        \"channels, media system, remote control issues, entertainment setup\"\n",
    "    ),\n",
    "    \"workspace\": (\n",
    "        \"workspace suitability, desk and chair, working remotely, laptop setup, \"\n",
    "        \"comfortable space for work or study, home office conditions\"\n",
    "    ),\n",
    "    \"climate_control\": (\n",
    "        \"air conditioning, heating system, temperature control, cooling or warmth, \"\n",
    "        \"poor ventilation, too hot or too cold inside the property\"\n",
    "    ),\n",
    "    \"furniture_and_comfort\": (\n",
    "        \"bed comfort, mattress quality, sofa or couch comfort, broken furniture, \"\n",
    "        \"uncomfortable seating, sleeping quality, worn or damaged furniture\"\n",
    "    ),\n",
    "    \"cleanliness\": (\n",
    "        \"cleanliness and hygiene, dirty or dusty apartment, unpleasant smells, \"\n",
    "        \"stains, poor cleaning standards, lack of sanitation\"\n",
    "    ),\n",
    "    \"linens\": (\n",
    "        \"bed linens, sheets, towels, blankets, linen cleanliness or freshness, \"\n",
    "        \"old, dirty, or insufficient towels or bedding\"\n",
    "    ),\n",
    "    \"bathroom_and_water\": (\n",
    "        \"bathroom condition, shower quality, toilet functionality, water pressure, \"\n",
    "        \"hot water availability, plumbing problems, mold or mildew in bathroom\"\n",
    "    ),\n",
    "    \"noise\": (\n",
    "        \"noise levels, loud neighbors, street noise, construction sounds, \"\n",
    "        \"poor sound insulation, difficult to sleep due to noise\"\n",
    "    ),\n",
    "    \"space_and_privacy\": (\n",
    "        \"apartment size, cramped or small space, layout and room arrangement, \"\n",
    "        \"lack of privacy, shared spaces, inconvenient floor plan or room separation\"\n",
    "    ),\n",
    "    \"kitchen_and_appliances\": (\n",
    "        \"kitchen facilities, cooking equipment, stove, oven, refrigerator, microwave, \"\n",
    "        \"coffee machine, kettle, broken or missing kitchen appliances\"\n",
    "    ),\n",
    "    \"rules_and_access\": (\n",
    "        \"check-in and check-out process, access to the property, keys or door codes, \"\n",
    "        \"house rules, unclear instructions, restrictions, confusing entry or exit\"\n",
    "    ),\n",
    "    \"parking\": (\n",
    "        \"parking availability, parking convenience, garage access, street parking, \"\n",
    "        \"difficulty finding parking near the property\"\n",
    "    ),\n",
    "    \"accessibility\": (\n",
    "        \"accessibility issues, elevator or lift availability, stairs only access, \"\n",
    "        \"difficulty reaching the apartment, suitability for limited mobility\"\n",
    "    ),\n",
    "    \"location\": (\n",
    "        \"location convenience, neighborhood quality, distance to attractions, \"\n",
    "        \"public transportation access, safety or comfort of the surrounding area\"\n",
    "    ),\n",
    "    \"view\": (\n",
    "        \"view from the property, balcony or window scenery, sea or city view, \"\n",
    "        \"blocked or disappointing view compared to listing description\"\n",
    "    ),\n",
    "    \"safety\": (\n",
    "        \"safety and security concerns, feeling unsafe, poor locks, building security, \"\n",
    "        \"unsafe neighborhood or entrance\"\n",
    "    ),\n",
    "    \"property_condition\": (\n",
    "        \"maintenance issues, broken items, poor condition, lack of repairs, \"\n",
    "        \"lighting problems, insufficient storage space, overall property upkeep\"\n",
    "    ),\n",
    "    \"host_communication\": (\n",
    "        \"communication with the host, slow or no response, lack of helpfulness, \"\n",
    "        \"difficulty resolving problems, unresponsive or unclear communication\"\n",
    "    ),\n",
    "    \"pets\": (\n",
    "        \"pet friendliness, pet restrictions, pet fees, issues with pets, \"\n",
    "        \"unexpected rules regarding animals\"\n",
    "    ),\n",
    "    \"price_value\": (\n",
    "        \"value for money, overpriced stay, price not matching quality, \"\n",
    "        \"expectations versus cost, poor value compared to similar listings\"\n",
    "    ),\n",
    "}\n",
    "ISSUES = list(ISSUE_TEXTS.keys())\n",
    "print(\"[INFO] #ISSUES =\", len(ISSUES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bee8353-29e6-4ea5-9b40-dce6fc5f97c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 0 - Base table: extract seller_id + drop nulls + (property_id, reviews) block-dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae1f4a5d-220d-4fd3-8cdd-ca7daadcd12e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "df_base = df_italy\n",
    "\n",
    "# Validate minimal inputs exist \n",
    "missing_inputs = [c for c in [PROPERTY_COL, REVIEWS_COL, SELLER_INFO_COL]if c not in df_base.columns]\n",
    "if missing_inputs:\n",
    "    raise Exception(f\"[ERROR] Missing required columns in airbnb: {missing_inputs}\\nFound: {df_base.columns}\")\n",
    "\n",
    "# Extract seller_id from seller_info JSON \n",
    "seller_schema = StructType([ StructField(\"seller_id\", StringType(), True), StructField(\"url\", StringType(), True), StructField(\"name\", StringType(), True), ])\n",
    "df_base = df_base.withColumn( \"seller_parsed\", F.from_json(F.col(SELLER_INFO_COL).cast(\"string\"), seller_schema) ).withColumn( SELLER_ID_COL, F.col(\"seller_parsed.seller_id\") ).drop(\"seller_parsed\")\n",
    "print(\"[OK] seller_id extracted from seller_info\")\n",
    "\n",
    "# Drop rows with missing critical keys/text\n",
    "df_base = df_base.filter( F.col(PROPERTY_COL).isNotNull() & (F.length(F.col(PROPERTY_COL).cast(\"string\")) > 0) & F.col(SELLER_ID_COL).isNotNull() & (F.length(F.col(SELLER_ID_COL).cast(\"string\")) > 0) & F.col(REVIEWS_COL).isNotNull() & (F.length(F.trim(F.col(REVIEWS_COL).cast(\"string\"))) > 0) )\n",
    "\n",
    "# Keep only required cols downstream\n",
    "df_base = df_base.select(PROPERTY_COL, SELLER_ID_COL, REVIEWS_COL)\n",
    "\n",
    "# Basic quality checks (after filtering)\n",
    "total_rows = df_base.count()\n",
    "print(\"[INFO] rows (after dropping null keys/text):\", total_rows)\n",
    "print(\"[INFO] unique properties:\",df_base.select(PROPERTY_COL).distinct().count())\n",
    "print(\"[INFO] unique sellers:\", df_base.select(SELLER_ID_COL).distinct().count())\n",
    "print(\"[DEBUG] sample rows:\")\n",
    "display(df_base.limit(10))\n",
    "\n",
    "# Save base table for the pipeline  \n",
    "save_table_and_path(df_base, T_REVIEWS_BASE)\n",
    "print(\"[INFO] base rows:\", df_base.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7caacf8-5603-4805-88be-6a57defb874f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove duplicated reviews inside the same property before sentence splitting.\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Output table for this step\n",
    "T_REVIEW_CHUNKS_BASE = f\"airbnb_review_chunks_base_{RUN_ID}\"\n",
    "print(\"[INFO] Output:\", T_REVIEW_CHUNKS_BASE)\n",
    "\n",
    "df0 = spark.table(T_REVIEWS_BASE)\n",
    "\n",
    "# Basic checks\n",
    "print(\"[INFO] T_REVIEWS_BASE rows:\", df0.count())\n",
    "display(df0.select(PROPERTY_COL, SELLER_ID_COL, REVIEWS_COL).limit(5))\n",
    "\n",
    "# Schema for occurrences (before dedup)\n",
    "chunks_schema = StructType([\n",
    "    StructField(PROPERTY_COL, StringType(), True),\n",
    "    StructField(SELLER_ID_COL, StringType(), True),\n",
    "    StructField(\"review_chunk\", StringType(), True),\n",
    "])\n",
    "\n",
    "def split_reviews_to_chunks(pdf_iter):\n",
    "    \"\"\"\n",
    "    Split the 'reviews' blob into individual reviews FIRST.\n",
    "    Primary rule: split on the delimiter  \",\"  (typical for scraped list-of-quotes reviews).\n",
    "    Fallback: paragraph/newline/bullets split.\n",
    "    \"\"\"\n",
    "    # Detect \"quoted reviews list\" delimiter:  \",\"  (allow whitespace/newlines between)\n",
    "    # Example: ...hotspots.\",\"A perfect stay...<br/>Straight down...\n",
    "    review_delim_re = re.compile(r'\"\\s*,\\s*\"')\n",
    "\n",
    "    # Fallback paragraph-ish split (kept from your original logic, slightly simplified)\n",
    "    fallback_split_re = re.compile(r\"(?:\\n\\s*\\n)+|(?:\\n[\\-\\*\\u2022]\\s+)|(?:\\n{1,})\")\n",
    "\n",
    "    # HTML <br/> -> newline\n",
    "    br_re = re.compile(r\"<br\\s*/?>\", flags=re.IGNORECASE)\n",
    "\n",
    "    # Strip outer wrappers like [\"...\",\"...\"] or leading/trailing quotes\n",
    "    strip_wrappers_re = re.compile(r'^\\s*\\[\\s*\"|\"\\s*\\]\\s*$')\n",
    "\n",
    "    for pdf in pdf_iter:\n",
    "        props = pdf[PROPERTY_COL].astype(str).tolist()\n",
    "        sellers = pdf[SELLER_ID_COL].astype(str).tolist()\n",
    "        blobs = pdf[REVIEWS_COL].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "        out = []\n",
    "        for i, blob in enumerate(blobs):\n",
    "            t = (blob or \"\").strip()\n",
    "            if not t:\n",
    "                continue\n",
    "\n",
    "            # normalize HTML breaks\n",
    "            t = br_re.sub(\"\\n\", t)\n",
    "\n",
    "            # If it looks like a quoted-list blob, split by \",\" between quotes\n",
    "            parts = []\n",
    "            if '\",\"' in t or review_delim_re.search(t):\n",
    "                # remove outer wrappers if exist\n",
    "                t2 = strip_wrappers_re.sub(\"\", t)\n",
    "                # split by delimiter\n",
    "                raw_parts = review_delim_re.split(t2)\n",
    "\n",
    "                # clean each review: strip stray quotes/commas/spaces\n",
    "                for p in raw_parts:\n",
    "                    p = (p or \"\").strip()\n",
    "                    p = p.strip(' \"')\n",
    "                    p = p.strip()\n",
    "                    if p:\n",
    "                        parts.append(p)\n",
    "\n",
    "            # Fallback: if delimiter logic didn’t produce anything\n",
    "            if not parts:\n",
    "                parts = [p.strip() for p in fallback_split_re.split(t) if p and p.strip()]\n",
    "                if not parts:\n",
    "                    parts = [t]\n",
    "\n",
    "            for ch in parts:\n",
    "                ch = re.sub(r\"\\s+\", \" \", ch).strip()\n",
    "                if len(ch) < 20:\n",
    "                    continue\n",
    "                out.append({\n",
    "                    PROPERTY_COL: props[i],\n",
    "                    SELLER_ID_COL: sellers[i],\n",
    "                    \"review_chunk\": ch\n",
    "                })\n",
    "\n",
    "        yield pd.DataFrame(out, columns=[PROPERTY_COL, SELLER_ID_COL, \"review_chunk\"])\n",
    "\n",
    "# 1) Split to chunk occurrences\n",
    "df_chunks_occ = (\n",
    "    df0.select(PROPERTY_COL, SELLER_ID_COL, REVIEWS_COL)\n",
    "       .repartition(400, SELLER_ID_COL)\n",
    "       .mapInPandas(split_reviews_to_chunks, schema=chunks_schema)\n",
    ")\n",
    "\n",
    "rows_chunks_occ = df_chunks_occ.count()\n",
    "print(f\"[INFO] review_chunk occurrences (before dedup): {rows_chunks_occ:,}\")\n",
    "display(df_chunks_occ.select(F.length(\"review_chunk\").alias(\"len\")).summary(\"min\",\"25%\",\"50%\",\"mean\",\"75%\",\"max\"))\n",
    "\n",
    "# 2) Normalize + dedup per (property_id, review_chunk_norm)\n",
    "df_chunks_occ = df_chunks_occ.withColumn(\"review_chunk\", F.trim(F.col(\"review_chunk\")))\n",
    "df_chunks_occ = df_chunks_occ.withColumn(\"review_chunk\", F.regexp_replace(F.col(\"review_chunk\"), r\"\\s+\", \" \"))\n",
    "\n",
    "df_chunks_occ = df_chunks_occ.withColumn(\"review_chunk_norm\", F.lower(F.col(\"review_chunk\")))\n",
    "df_chunks_occ = df_chunks_occ.withColumn(\"review_chunk_norm\", F.regexp_replace(F.col(\"review_chunk_norm\"), r\"\\s+\", \" \"))\n",
    "df_chunks_occ = df_chunks_occ.withColumn(\"review_chunk_norm\", F.trim(F.col(\"review_chunk_norm\")))\n",
    "\n",
    "# Apply dedup\n",
    "df_chunks_unique = df_chunks_occ.dropDuplicates([PROPERTY_COL, \"review_chunk_norm\"])\n",
    "\n",
    "rows_chunks_unique = df_chunks_unique.count()\n",
    "removed = rows_chunks_occ - rows_chunks_unique\n",
    "print(f\"[OK] Chunk dedup done: unique={rows_chunks_unique:,} | removed={removed:,} ({(removed/rows_chunks_occ):.2%} saved)\")\n",
    "\n",
    "# QA: show properties with most removed chunks\n",
    "prop_before = df_chunks_occ.groupBy(PROPERTY_COL).count().withColumnRenamed(\"count\", \"chunks_before\")\n",
    "prop_after  = df_chunks_unique.groupBy(PROPERTY_COL).count().withColumnRenamed(\"count\", \"chunks_after\")\n",
    "\n",
    "prop_delta = (\n",
    "    prop_before.join(prop_after, on=PROPERTY_COL, how=\"inner\")\n",
    "               .withColumn(\"removed_chunks\", F.col(\"chunks_before\") - F.col(\"chunks_after\"))\n",
    "               .orderBy(F.desc(\"removed_chunks\"))\n",
    ")\n",
    "\n",
    "print(\"[DEBUG] Properties with most duplicated chunks removed:\")\n",
    "display(prop_delta.limit(30))\n",
    "\n",
    "# 3) Add stable ids for downstream joins / traceability\n",
    "df_chunks_unique = df_chunks_unique.withColumn(\"review_chunk_id\", F.sha2(F.col(\"review_chunk_norm\"), 256))\n",
    "\n",
    "# Keep only needed cols\n",
    "df_chunks_unique = df_chunks_unique.select(\n",
    "    PROPERTY_COL, SELLER_ID_COL,\n",
    "    \"review_chunk_id\",\n",
    "    \"review_chunk\",\n",
    "    \"review_chunk_norm\"\n",
    ")\n",
    "\n",
    "# 4) Save\n",
    "save_table_and_path(df_chunks_unique, T_REVIEW_CHUNKS_BASE)\n",
    "print(\"[INFO] review_chunks unique:\", df_chunks_unique.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1760ef8-4710-4939-840f-8d1921c98e4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"[QA] Example raw reviews blob (first row):\")\n",
    "display(df0.select(REVIEWS_COL).limit(1))\n",
    "\n",
    "print(\"[QA] Sample produced review_chunks:\")\n",
    "display(df_chunks_occ.select(PROPERTY_COL, \"review_chunk\", F.length(\"review_chunk\").alias(\"len\")).limit(10))\n",
    "\n",
    "print(\"[QA] How many chunks per property (top 20):\")\n",
    "display(\n",
    "    df_chunks_occ.groupBy(PROPERTY_COL)\n",
    "                 .agg(F.count(\"*\").alias(\"n_chunks\"))\n",
    "                 .orderBy(F.desc(\"n_chunks\"))\n",
    "                 .limit(20)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34eece7a-079b-4de5-8dfc-31f73dda296a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1 - Sentence splitting + English-only + min_words + smart normalization (for dedup)\n",
    "**Outputs:**\n",
    "- `T_SENTENCES_OCCUR`: all sentence occurrences (keeps property/host linkage)\n",
    "- `T_SENTENCES_UNIQUE`: unique sentences by `sentence_norm` for model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9041a4b3-b1c3-4134-892b-fbb02a0f6a52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import pandas as pd\n",
    "\n",
    "TEXT_COL = \"review_chunk\"\n",
    "\n",
    "occur_schema = StructType([\n",
    "    StructField(PROPERTY_COL, StringType(), True),\n",
    "    StructField(SELLER_ID_COL, StringType(), True),\n",
    "    StructField(\"review_chunk_id\", StringType(), True),\n",
    "    StructField(\"sentence\", StringType(), True),\n",
    "])\n",
    "\n",
    "def split_to_sentences(pdf_iter):\n",
    "    import re\n",
    "    splitter = re.compile(r\"(?<=[\\.\\!\\?])\\s+|\\n+\")\n",
    "\n",
    "    for pdf in pdf_iter:\n",
    "        out_rows = []\n",
    "        prop   = pdf[PROPERTY_COL].astype(str).tolist()\n",
    "        seller = pdf[SELLER_ID_COL].astype(str).tolist()\n",
    "        cid    = pdf[\"review_chunk_id\"].astype(str).tolist()\n",
    "        texts  = pdf[TEXT_COL].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            t = (text or \"\").strip()\n",
    "            if not t:\n",
    "                continue\n",
    "\n",
    "            parts = [p.strip() for p in splitter.split(t) if p and p.strip()]\n",
    "            for s in parts:\n",
    "                out_rows.append({\n",
    "                    PROPERTY_COL: prop[i],\n",
    "                    SELLER_ID_COL: seller[i],\n",
    "                    \"review_chunk_id\": cid[i],\n",
    "                    \"sentence\": s\n",
    "                })\n",
    "\n",
    "        yield pd.DataFrame(out_rows, columns=[PROPERTY_COL, SELLER_ID_COL, \"review_chunk_id\", \"sentence\"])\n",
    "\n",
    "\n",
    "df_in = spark.table(T_REVIEW_CHUNKS_BASE)\n",
    "\n",
    "df_occ = (\n",
    "    df_in\n",
    "    .select(PROPERTY_COL, SELLER_ID_COL, \"review_chunk_id\", F.col(TEXT_COL).alias(TEXT_COL))\n",
    "    .repartition(400, SELLER_ID_COL)\n",
    "    .mapInPandas(split_to_sentences, schema=occur_schema)\n",
    ")\n",
    "\n",
    "# clean\n",
    "df_occ = df_occ.withColumn(\"sentence\", F.trim(F.col(\"sentence\")))\n",
    "df_occ = df_occ.withColumn(\"sentence\", F.regexp_replace(F.col(\"sentence\"), r\"\\s+\", \" \"))\n",
    "\n",
    "# min_words\n",
    "df_occ = df_occ.withColumn(\"n_words\", F.size(F.split(F.col(\"sentence\"), r\"\\s+\")))\n",
    "df_occ = df_occ.filter(F.col(\"n_words\") >= F.lit(int(MIN_WORDS)))\n",
    "\n",
    "# english-only heuristic (strict ASCII + some latin letters)\n",
    "df_occ = df_occ.withColumn(\"non_ascii\", F.length(F.regexp_replace(F.col(\"sentence\"), r\"[\\x00-\\x7F]\", \"\")))\n",
    "df_occ = df_occ.withColumn(\"latin_letters\", F.length(F.regexp_replace(F.col(\"sentence\"), r\"[^A-Za-z]\", \"\")))\n",
    "df_occ = df_occ.filter((F.col(\"non_ascii\") <= F.lit(0)) & (F.col(\"latin_letters\") >= F.lit(3)))\n",
    "\n",
    "# norm for smart dedup\n",
    "df_occ = df_occ.withColumn(\"sentence_norm\", F.lower(F.col(\"sentence\")))\n",
    "df_occ = df_occ.withColumn(\"sentence_norm\", F.regexp_replace(F.col(\"sentence_norm\"), r\"\\s+\", \" \"))\n",
    "df_occ = df_occ.withColumn(\"sentence_norm\", F.regexp_replace(F.col(\"sentence_norm\"), r\"^[\\.\\,\\!\\?\\:\\;\\-]+\", \"\"))\n",
    "df_occ = df_occ.withColumn(\"sentence_norm\", F.regexp_replace(F.col(\"sentence_norm\"), r\"[\\.\\,\\!\\?\\:\\;\\-]+$\", \"\"))\n",
    "df_occ = df_occ.withColumn(\"sentence_norm\", F.trim(F.col(\"sentence_norm\")))\n",
    "\n",
    "# stable id\n",
    "df_occ = df_occ.withColumn(\"sentence_id\", F.sha2(F.col(\"sentence_norm\"), 256))\n",
    "\n",
    "save_table_and_path(\n",
    "    df_occ.select(PROPERTY_COL, SELLER_ID_COL, \"review_chunk_id\",\n",
    "                  \"sentence_id\", \"sentence\", \"sentence_norm\", \"n_words\"),\n",
    "    T_SENTENCES_OCCUR\n",
    ")\n",
    "\n",
    "df_unique = (\n",
    "    spark.table(T_SENTENCES_OCCUR)\n",
    "         .select(\"sentence_id\", \"sentence\", \"sentence_norm\")\n",
    "         .dropDuplicates([\"sentence_norm\"])\n",
    ")\n",
    "save_table_and_path(df_unique, T_SENTENCES_UNIQUE)\n",
    "\n",
    "print(\"[INFO] sentences_occur:\", spark.table(T_SENTENCES_OCCUR).count())\n",
    "print(\"[INFO] sentences_unique:\", spark.table(T_SENTENCES_UNIQUE).count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f308bceb-cadb-4b1b-be80-07a335c01a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2 - Candidate Sentence Prefilter (Top-K)\n",
    "Selects a small, high-signal subset of review sentences per property for downstream semantic matching and API processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "105fa109-4398-4941-a1b6-c5370b77e62e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Params\n",
    "K_HI = 25\n",
    "K_FALLBACK = 15\n",
    "TARGET_MAX = K_HI + K_FALLBACK\n",
    "\n",
    "# Load occurrences\n",
    "df_occ = spark.table(T_SENTENCES_OCCUR).select(\n",
    "    PROPERTY_COL, SELLER_ID_COL, \"sentence_id\", \"sentence\", \"sentence_norm\", \"n_words\"\n",
    ")\n",
    "\n",
    "# High-signal heuristic \n",
    "HIGH_SIGNAL_TERMS = [\n",
    "    # negation / mismatch\n",
    "    \"not\", \"no\", \"never\", \"without\", \"can't\", \"cannot\", \"didn't\", \"doesn't\", \"wasn't\", \"weren't\",\n",
    "    \"as described\", \"not as\", \"different than\", \"nothing like\", \"misleading\", \"false advertising\",\n",
    "\n",
    "    # complaints / dissatisfaction\n",
    "    \"complaint\", \"complain\", \"disappointed\", \"disappointing\", \"unhappy\", \"regret\", \"avoid\",\n",
    "    \"terrible\", \"awful\", \"horrible\", \"worst\", \"bad\", \"poor\", \"mediocre\", \"unacceptable\",\n",
    "\n",
    "    # service / communication\n",
    "    \"rude\", \"unresponsive\", \"ignored\", \"no response\", \"didn't respond\", \"host didn't\",\n",
    "    \"late\", \"delay\", \"delayed\", \"cancelled\", \"canceled\",\n",
    "\n",
    "    # functionality / maintenance\n",
    "    \"broken\", \"not working\", \"doesn't work\", \"didn't work\", \"failed\", \"issue\", \"problem\",\n",
    "    \"malfunction\", \"leak\", \"leaking\", \"clogged\", \"blocked\", \"stuck\", \"damaged\",\n",
    "\n",
    "    # cleanliness / hygiene\n",
    "    \"dirty\", \"filthy\", \"unclean\", \"smell\", \"smelly\", \"odor\", \"stains\", \"mold\", \"mildew\",\n",
    "    \"bugs\", \"cockroach\", \"roaches\", \"insects\", \"ants\", \"bedbug\", \"bed bugs\", \"hair\",\n",
    "\n",
    "    # noise / safety\n",
    "    \"noisy\", \"loud\", \"noise\", \"unsafe\", \"dangerous\", \"scary\",\n",
    "\n",
    "    # value / pricing\n",
    "    \"overpriced\", \"not worth\", \"waste\", \"ripoff\", \"scam\", \"expensive\",\n",
    "\n",
    "    # comfort / sleep / climate\n",
    "    \"uncomfortable\", \"hard bed\", \"bed hard\", \"small\", \"tiny\", \"cramped\",\n",
    "    \"cold\", \"freezing\", \"hot\", \"heat\", \"ac\", \"air conditioning\", \"air conditioner\", \"no ac\",\n",
    "    \"heater\", \"no heat\", \"no hot water\", \"hot water\",\n",
    "]\n",
    "\n",
    "def _mk_or_regex(terms):\n",
    "    import re\n",
    "    return \"(\" + \"|\".join(re.escape(t) for t in terms) + \")\"\n",
    "\n",
    "df = (\n",
    "    df_occ\n",
    "    .withColumn(\"sent_lc\", F.col(\"sentence_norm\"))\n",
    "    .withColumn(\"is_high_signal\", F.col(\"sent_lc\").rlike(_mk_or_regex(HIGH_SIGNAL_TERMS)))\n",
    ")\n",
    "\n",
    "# pick HI per property\n",
    "w_hi = Window.partitionBy(PROPERTY_COL).orderBy(F.desc(\"is_high_signal\"), F.desc(\"n_words\"))\n",
    "pick_hi = (\n",
    "    df.filter(F.col(\"is_high_signal\") == True)\n",
    "      .withColumn(\"rk\", F.row_number().over(w_hi))\n",
    "      .filter(F.col(\"rk\") <= F.lit(int(K_HI)))\n",
    "      .drop(\"rk\")\n",
    ")\n",
    "\n",
    "# fallback: fill remaining budget by n_words\n",
    "seed_keys = pick_hi.select(PROPERTY_COL, \"sentence_norm\").dropDuplicates()\n",
    "seed_counts = pick_hi.groupBy(PROPERTY_COL).agg(F.count(\"*\").alias(\"seed_n\"))\n",
    "\n",
    "df_fb = (\n",
    "    df.join(seed_counts, on=PROPERTY_COL, how=\"left\").fillna({\"seed_n\": 0})\n",
    "      .join(seed_keys, on=[PROPERTY_COL, \"sentence_norm\"], how=\"left_anti\")\n",
    "      .withColumn(\n",
    "          \"need_n\",\n",
    "          F.when(F.col(\"seed_n\") >= F.lit(TARGET_MAX), F.lit(0))\n",
    "           .otherwise(F.lit(TARGET_MAX) - F.col(\"seed_n\"))\n",
    "      )\n",
    "      .withColumn(\"need_n\", F.least(F.col(\"need_n\"), F.lit(int(K_FALLBACK))))\n",
    ")\n",
    "\n",
    "w_fb = Window.partitionBy(PROPERTY_COL).orderBy(F.desc(\"n_words\"))\n",
    "pick_fb = (\n",
    "    df_fb.withColumn(\"rk\", F.row_number().over(w_fb))\n",
    "         .filter(F.col(\"rk\") <= F.col(\"need_n\"))\n",
    "         .drop(\"rk\", \"need_n\", \"seed_n\")\n",
    ")\n",
    "\n",
    "# Final candidates + save\n",
    "df_cand = pick_hi.unionByName(pick_fb).dropDuplicates([PROPERTY_COL, \"sentence_norm\"])\n",
    "save_table_and_path(df_cand, T_SENTENCES_CANDIDATES_OCCUR)\n",
    "\n",
    "df_cand_unique = (\n",
    "    df_cand.select(\"sentence_id\", \"sentence\", \"sentence_norm\")\n",
    "           .dropDuplicates([\"sentence_norm\"])\n",
    ")\n",
    "save_table_and_path(df_cand_unique, T_SENTENCES_CANDIDATES_UNIQUE)\n",
    "\n",
    "print(\"[INFO] prefilter in:\", df_occ.count(), \"| out:\", df_cand.count())\n",
    "print(\"[INFO] prefilter unique:\", df_cand_unique.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e986be6-82e5-4ba4-ba46-2d45ce42db69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3 – Semantic Similarity-Based Issue Matching\n",
    "We compute Top1/Top2 + gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3765909-64e0-47af-b890-33310bd597b2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"#row_number#\":112},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768388091624}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "sim_schema = StructType([\n",
    "    StructField(\"sentence_id\", StringType(), True),\n",
    "    StructField(\"sentence\", StringType(), True),\n",
    "    StructField(\"sentence_norm\", StringType(), True),\n",
    "\n",
    "    StructField(\"top1_issue\", StringType(), True),\n",
    "    StructField(\"top1_sim\", FloatType(), True),\n",
    "    StructField(\"top2_issue\", StringType(), True),\n",
    "    StructField(\"top2_sim\", FloatType(), True),\n",
    "    StructField(\"top_gap\", FloatType(), True),\n",
    "\n",
    "    StructField(\"sim_model\", StringType(), True),\n",
    "])\n",
    "\n",
    "def make_similarity_once_fn_fast(model_name: str, issue_texts: dict, batch_size: int = 256, issue_batch: int = 64):\n",
    "    def _fn(pdf_iter):\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        from functools import lru_cache\n",
    "\n",
    "        # Prevent CPU thread explosion per executor\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        @lru_cache(maxsize=1)\n",
    "        def get_model_and_issue_emb():\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            # force cpu (since you don't have cuda currently)\n",
    "            model = SentenceTransformer(model_name, device=\"cpu\")\n",
    "            issues = list(issue_texts.keys())\n",
    "            issue_desc = [issue_texts[k] for k in issues]\n",
    "            issue_emb = model.encode(\n",
    "                issue_desc,\n",
    "                normalize_embeddings=True,\n",
    "                batch_size=issue_batch,\n",
    "                show_progress_bar=False,\n",
    "            )\n",
    "            return model, issues, np.asarray(issue_emb, dtype=np.float32)\n",
    "\n",
    "        model, issues, issue_emb = get_model_and_issue_emb()\n",
    "\n",
    "        for pdf in pdf_iter:\n",
    "            sents = pdf[\"sentence\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "            emb = model.encode(\n",
    "                sents,\n",
    "                normalize_embeddings=True,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=False,\n",
    "            )\n",
    "            emb = np.asarray(emb, dtype=np.float32)\n",
    "\n",
    "            sims = emb @ issue_emb.T  # cosine similarity because normalized\n",
    "\n",
    "            # Fast top-2 (no full sort)\n",
    "            top2 = np.argpartition(-sims, kth=1, axis=1)[:, :2]\n",
    "            row = np.arange(sims.shape[0])\n",
    "            a = top2[:, 0]\n",
    "            b = top2[:, 1]\n",
    "            a_better = sims[row, a] >= sims[row, b]\n",
    "            top1_i = np.where(a_better, a, b)\n",
    "            top2_i = np.where(a_better, b, a)\n",
    "\n",
    "            top1_sim = sims[row, top1_i].astype(np.float32)\n",
    "            top2_sim = sims[row, top2_i].astype(np.float32)\n",
    "            top_gap = (top1_sim - top2_sim).astype(np.float32)\n",
    "\n",
    "            out = pdf[[\"sentence_id\", \"sentence\", \"sentence_norm\"]].copy()\n",
    "            out[\"top1_issue\"] = [issues[i] for i in top1_i]\n",
    "            out[\"top1_sim\"] = top1_sim\n",
    "            out[\"top2_issue\"] = [issues[i] for i in top2_i]\n",
    "            out[\"top2_sim\"] = top2_sim\n",
    "            out[\"top_gap\"] = top_gap\n",
    "            out[\"sim_model\"] = \"minilm\"\n",
    "            yield out\n",
    "    return _fn\n",
    "\n",
    "df_unique = spark.table(T_SENTENCES_CANDIDATES_UNIQUE)\n",
    "\n",
    "# choose partitions sanely for CPU\n",
    "N = max(32, min(128, spark.sparkContext.defaultParallelism * 2))\n",
    "print(\"[INFO] Similarity partitions:\", N)\n",
    "\n",
    "fn_sim_once = make_similarity_once_fn_fast(SIM_MODEL_NAME, ISSUE_TEXTS, batch_size=128)\n",
    "\n",
    "df_sim = (\n",
    "    df_unique\n",
    "    .repartition(N, \"sentence_id\")\n",
    "    .mapInPandas(fn_sim_once, schema=sim_schema)\n",
    ")\n",
    "\n",
    "save_table_and_path(df_sim, T_SIM_UNIQUE)\n",
    "print(\"[INFO] sim rows:\", spark.table(T_SIM_UNIQUE).count())\n",
    "\n",
    "display(spark.table(T_SIM_UNIQUE)\n",
    "       .select(\"top1_sim\",\"top2_sim\",\"top_gap\")\n",
    "       .sample(0.001, seed=42)\n",
    "       .summary(\"min\",\"50%\",\"mean\",\"max\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25011c19-ccac-427b-be86-9ee9532f9080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Apply similarity thresholds (Top1 + optional Top2) BEFORE sentiment\n",
    "We define candidate issue tags from similarity only.\n",
    "\n",
    "**Rules:**\n",
    "- Keep Top1 if `top1_sim >= ISSUE_SIM_THRESHOLD`\n",
    "- Add Top2 only if `top2_sim >= ISSUE_SIM_THRESHOLD` AND `top_gap <= TOP2_GAP_MAX`\n",
    "- `TOP_GAP_MIN` is QA signal (we keep it for analysis, not for hard drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a635af04-b40d-4f3a-b74c-64caf8cea99a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sim = spark.table(T_SIM_UNIQUE)\n",
    "\n",
    "# Top1 candidates\n",
    "df_top1 = (\n",
    "    df_sim\n",
    "    .filter(F.col(\"top1_sim\") >= F.lit(float(ISSUE_SIM_THRESHOLD)))\n",
    "    .select(\n",
    "        \"sentence_id\",\"sentence\",\"sentence_norm\",\n",
    "        \"top1_issue\",\"top1_sim\",\"top2_issue\",\"top2_sim\",\"top_gap\",\"sim_model\",\n",
    "        F.col(\"top1_issue\").alias(\"issue\"),\n",
    "        F.col(\"top1_sim\").alias(\"issue_sim\"),\n",
    "        F.lit(False).alias(\"is_top2\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Top2 candidates (only when very close)\n",
    "df_top2 = (\n",
    "    df_sim\n",
    "    .filter((F.col(\"top2_sim\") >= F.lit(float(ISSUE_SIM_THRESHOLD))) & (F.col(\"top_gap\") <= F.lit(float(TOP2_GAP_MAX))))\n",
    "    .select(\n",
    "        \"sentence_id\",\"sentence\",\"sentence_norm\",\n",
    "        \"top1_issue\",\"top1_sim\",\"top2_issue\",\"top2_sim\",\"top_gap\",\"sim_model\",\n",
    "        F.col(\"top2_issue\").alias(\"issue\"),\n",
    "        F.col(\"top2_sim\").alias(\"issue_sim\"),\n",
    "        F.lit(True).alias(\"is_top2\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_issues_candidates = df_top1.unionByName(df_top2)\n",
    "\n",
    "save_table_and_path(df_issues_candidates, T_ISSUES_CANDIDATES_UNIQUE)\n",
    "print(\"[INFO] issue-candidate rows:\", spark.table(T_ISSUES_CANDIDATES_UNIQUE).count())\n",
    "\n",
    "icu = spark.table(T_ISSUES_CANDIDATES_UNIQUE).cache()\n",
    "display(icu.groupBy(\"issue\").count().orderBy(F.desc(\"count\")).limit(30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51e16808-bec3-4d01-a28d-ace65f966d65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4 - Sentiment (SST-2) ONLY on the similarity-candidates\n",
    "Keep where `neg_prob >= 0.40`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28cdf583-a306-45ea-a32b-3e3f12fc1b85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "N_SENT_SHARDS = 40\n",
    "T_SENT_INPUT = f\"airbnb_sent_input_{RUN_ID}\"\n",
    "T_SENT_PROGRESS = f\"airbnb_sent_progress_{RUN_ID}\"\n",
    "\n",
    "cand_issues = spark.table(T_ISSUES_CANDIDATES_UNIQUE).select(\"sentence_id\",\"sentence\").dropDuplicates([\"sentence_id\"])\n",
    "sent_input = (\n",
    "    cand_issues\n",
    "    .withColumn(\"sent_shard\", F.pmod(F.hash(F.col(\"sentence_id\")), F.lit(int(N_SENT_SHARDS))))\n",
    ")\n",
    "\n",
    "sent_input.write.mode(\"overwrite\").format(\"delta\").saveAsTable(T_SENT_INPUT)\n",
    "print(\"[OK] Saved:\", T_SENT_INPUT, \"| rows:\", spark.table(T_SENT_INPUT).count())\n",
    "\n",
    "# progress table (append-only)\n",
    "progress_schema = \"sent_shard INT, ts STRING, rows_scored LONG\"\n",
    "if not spark.catalog.tableExists(T_SENT_PROGRESS):\n",
    "    spark.createDataFrame([], progress_schema).write.mode(\"overwrite\").format(\"delta\").saveAsTable(T_SENT_PROGRESS)\n",
    "    print(\"[OK] Init progress table:\", T_SENT_PROGRESS)\n",
    "else:\n",
    "    print(\"[OK] Progress exists (resume):\", T_SENT_PROGRESS, \"| rows:\", spark.table(T_SENT_PROGRESS).count())\n",
    "print(\"[OK] Init progress table:\", T_SENT_PROGRESS)\n",
    "\n",
    "display(spark.table(T_SENT_INPUT).groupBy(\"sent_shard\").count().orderBy(F.desc(\"count\")).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3870d9fe-2c3c-45b8-97e8-c06ecd966989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "sent_schema = StructType([\n",
    "    StructField(\"sentence_id\", StringType(), True),\n",
    "    StructField(\"neg_prob\", FloatType(), True),\n",
    "    StructField(\"pos_prob\", FloatType(), True),\n",
    "    StructField(\"sentiment_label\", StringType(), True),\n",
    "    StructField(\"sent_model\", StringType(), True),\n",
    "])\n",
    "\n",
    "progress_schema = \"sent_shard INT, ts STRING, rows_scored LONG\"\n",
    "\n",
    "def make_sst2_fn(\n",
    "    model_name: str,\n",
    "    batch_size_cpu: int = 64,\n",
    "    batch_size_gpu: int = 256,\n",
    "    max_len: int = 128,\n",
    "):\n",
    "    def _fn(pdf_iter):\n",
    "        import os\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "        from functools import lru_cache\n",
    "\n",
    "        # reduce overhead / thread explosion\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "        torch.set_num_threads(1)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        bs = batch_size_gpu if device.type == \"cuda\" else batch_size_cpu\n",
    "\n",
    "        @lru_cache(maxsize=1)\n",
    "        def get_tok_model():\n",
    "            tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "            mdl = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "            mdl.eval()\n",
    "            return tok, mdl\n",
    "\n",
    "        tok, mdl = get_tok_model()\n",
    "        labels = [\"negative\", \"positive\"]\n",
    "\n",
    "        use_amp = (device.type == \"cuda\")\n",
    "        autocast_ctx = torch.cuda.amp.autocast if use_amp else None\n",
    "\n",
    "        for pdf in pdf_iter:\n",
    "            if pdf is None or pdf.empty:\n",
    "                yield pd.DataFrame(columns=[\"sentence_id\",\"neg_prob\",\"pos_prob\",\"sentiment_label\",\"sent_model\"])\n",
    "                continue\n",
    "\n",
    "            texts = pdf[\"sentence\"].fillna(\"\").astype(str).tolist()\n",
    "            ids   = pdf[\"sentence_id\"].astype(str).tolist()\n",
    "            n = len(texts)\n",
    "\n",
    "            neg = np.zeros(n, dtype=np.float32)\n",
    "            pos = np.zeros(n, dtype=np.float32)\n",
    "            lab = np.empty(n, dtype=object)\n",
    "\n",
    "            # faster than no_grad for inference\n",
    "            with torch.inference_mode():\n",
    "                for start in range(0, n, bs):\n",
    "                    end = min(n, start + bs)\n",
    "                    enc = tok(\n",
    "                        texts[start:end],\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        max_length=max_len,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "                    if use_amp:\n",
    "                        with autocast_ctx():\n",
    "                            logits = mdl(**enc).logits\n",
    "                    else:\n",
    "                        logits = mdl(**enc).logits\n",
    "\n",
    "                    probs = torch.softmax(logits, dim=-1).float().cpu().numpy()\n",
    "                    neg[start:end] = probs[:, 0]\n",
    "                    pos[start:end] = probs[:, 1]\n",
    "                    lab[start:end] = [labels[i] for i in probs.argmax(axis=1)]\n",
    "\n",
    "            yield pd.DataFrame({\n",
    "                \"sentence_id\": ids,\n",
    "                \"neg_prob\": neg,\n",
    "                \"pos_prob\": pos,\n",
    "                \"sentiment_label\": lab,\n",
    "                \"sent_model\": [\"sst2\"] * n\n",
    "            })\n",
    "    return _fn\n",
    "\n",
    "\n",
    "# Init output/progress only if missing (resume-safe)\n",
    "if not spark.catalog.tableExists(T_SENTIMENT_UNIQUE):\n",
    "    spark.createDataFrame([], sent_schema).write.mode(\"overwrite\").format(\"delta\").saveAsTable(T_SENTIMENT_UNIQUE)\n",
    "    print(\"[OK] Init output:\", T_SENTIMENT_UNIQUE)\n",
    "else:\n",
    "    print(\"[OK] Output exists (resume):\", T_SENTIMENT_UNIQUE)\n",
    "\n",
    "if not spark.catalog.tableExists(T_SENT_PROGRESS):\n",
    "    spark.createDataFrame([], progress_schema).write.mode(\"overwrite\").format(\"delta\").saveAsTable(T_SENT_PROGRESS)\n",
    "    print(\"[OK] Init progress:\", T_SENT_PROGRESS)\n",
    "else:\n",
    "    print(\"[OK] Progress exists (resume):\", T_SENT_PROGRESS)\n",
    "\n",
    "\n",
    "fn_sst2 = make_sst2_fn(SENT_MODEL_NAME)\n",
    "\n",
    "# Partitions: keep small to reduce overhead per shard \n",
    "P = max(1, min(8, spark.sparkContext.defaultParallelism * 2))\n",
    "print(\"[INFO] P:\", P, \"| defaultParallelism:\", spark.sparkContext.defaultParallelism)\n",
    "\n",
    "# Done shards\n",
    "done = set(r[\"sent_shard\"] for r in spark.table(T_SENT_PROGRESS).select(\"sent_shard\").distinct().collect())\n",
    "print(f\"[INFO] already done shards: {len(done)} / {N_SENT_SHARDS}\")\n",
    "\n",
    "# Optional health: show biggest shards\n",
    "print(\"[QA] Largest sentiment shards (top 10)\")\n",
    "display(\n",
    "    spark.table(T_SENT_INPUT)\n",
    "         .groupBy(\"sent_shard\")\n",
    "         .count()\n",
    "         .orderBy(F.desc(\"count\"))\n",
    "         .limit(10)\n",
    ")\n",
    "\n",
    "# Main loop\n",
    "for k in range(N_SENT_SHARDS):\n",
    "    if k in done:\n",
    "        continue\n",
    "\n",
    "    df_k = (\n",
    "        spark.table(T_SENT_INPUT)\n",
    "             .filter(F.col(\"sent_shard\") == F.lit(int(k)))\n",
    "             .select(\"sentence_id\",\"sentence\")\n",
    "    )\n",
    "\n",
    "    n_in = df_k.count()\n",
    "    print(f\"\\n[RUN] sent_shard={k} | input rows={n_in:,}\")\n",
    "    if n_in == 0:\n",
    "        spark.createDataFrame([(k, datetime.utcnow().isoformat(), 0)], progress_schema) \\\n",
    "            .write.mode(\"append\").format(\"delta\").saveAsTable(T_SENT_PROGRESS)\n",
    "        continue\n",
    "\n",
    "    scored = (\n",
    "        df_k.coalesce(P)\n",
    "            .mapInPandas(fn_sst2, schema=sent_schema)\n",
    "    )\n",
    "\n",
    "    scored.write.mode(\"append\").format(\"delta\").saveAsTable(T_SENTIMENT_UNIQUE)\n",
    "\n",
    "    spark.createDataFrame([(k, datetime.utcnow().isoformat(), n_in)], progress_schema) \\\n",
    "         .write.mode(\"append\").format(\"delta\").saveAsTable(T_SENT_PROGRESS)\n",
    "\n",
    "    # light prints: do NOT count the whole output table each shard\n",
    "    if (k % 10) == 0:\n",
    "        done_now = spark.table(T_SENT_PROGRESS).select(\"sent_shard\").distinct().count()\n",
    "        out_rows = spark.table(T_SENT_PROGRESS).agg(F.sum(\"rows_scored\").alias(\"rows\")).collect()[0][\"rows\"]\n",
    "        print(f\"[OK] shard {k} appended | approx_rows_scored={out_rows:,} | shards_done={done_now}/{N_SENT_SHARDS}\")\n",
    "    else:\n",
    "        print(f\"[OK] shard {k} appended | shards_done=? (see every 10 shards)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d506926-1ca4-453a-bd28-043a3d3e1e38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Build NEG_UNIQUE (neg_prob >= threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8942a349-0ad0-4595-8065-379f14a17638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "sent = spark.table(T_SENTIMENT_UNIQUE)\n",
    "\n",
    "neg_unique = (\n",
    "    sent.filter(F.col(\"neg_prob\") >= F.lit(float(NEG_PROB_THRESHOLD)))\n",
    "        .select(\"sentence_id\",\"neg_prob\",\"pos_prob\",\"sentiment_label\",\"sent_model\")\n",
    "        .dropDuplicates([\"sentence_id\"])\n",
    ")\n",
    "\n",
    "save_table_and_path(neg_unique, T_NEG_UNIQUE)\n",
    "print(\"[INFO] neg_unique rows:\", spark.table(T_NEG_UNIQUE).count())\n",
    "display(neg_unique.select(\"neg_prob\").summary(\"min\",\"25%\",\"50%\",\"mean\",\"75%\",\"max\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca1082a0-fb3f-4b38-8db8-1acb213147b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5 - Join: (Issue candidates) × (Negative sentiment) → final issues per sentence\n",
    "Then map back to occurrences to preserve property/host linkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf87060-e5ba-4456-b051-7fc0e226f1e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "issues = spark.table(T_ISSUES_CANDIDATES_UNIQUE)\n",
    "neg = spark.table(T_NEG_UNIQUE)\n",
    "\n",
    "df_final_issues_unique = (\n",
    "    issues.join(neg.select(\"sentence_id\",\"neg_prob\",\"pos_prob\",\"sentiment_label\"), on=\"sentence_id\", how=\"inner\")\n",
    ")\n",
    "\n",
    "save_table_and_path(df_final_issues_unique, T_ISSUES_UNIQUE)\n",
    "iu2 = spark.table(T_ISSUES_UNIQUE)\n",
    "print(\"[OK] Final issues unique saved:\", T_ISSUES_UNIQUE, \"| rows:\", iu2.count())\n",
    "\n",
    "print(\"[OK] Final issues unique saved (overwrites candidates):\", T_ISSUES_UNIQUE, \"| rows:\", iu2.count())\n",
    "display(iu2.groupBy(\"issue\").count().orderBy(F.desc(\"count\")).limit(30))\n",
    "display(iu2.orderBy(F.desc(\"neg_prob\")).select(\"neg_prob\",\"issue\",\"issue_sim\",\"sentence\").limit(30))\n",
    "\n",
    "# Map to occurrences\n",
    "occ = spark.table(T_SENTENCES_CANDIDATES_OCCUR)\n",
    "\n",
    "df_issues_occur = occ.join(iu2.select(\"sentence_id\",\"issue\",\"issue_sim\",\"is_top2\",\"top_gap\",\"top1_issue\",\"top2_issue\",\"top1_sim\",\"top2_sim\",\"neg_prob\"), on=\"sentence_id\", how=\"inner\")\n",
    "\n",
    "save_table_and_path(df_issues_occur, T_ISSUES_OCCUR)\n",
    "\n",
    "io = spark.table(T_ISSUES_OCCUR).cache()\n",
    "print(\"[OK] Saved:\", T_ISSUES_OCCUR, \"| rows:\", io.count())\n",
    "display(io.groupBy(\"issue\").count().orderBy(F.desc(\"count\")).limit(30))\n",
    "display(io.orderBy(F.desc(\"issue_sim\")).select(PROPERTY_COL, SELLER_ID_COL, \"issue\", \"issue_sim\", \"neg_prob\", \"sentence\").limit(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "805c9f12-1a06-49a6-afb6-d8ba57fb2989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6 - Aggregations (Property + Host) + Evidence tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3abdf8-1686-461f-940a-5dd32da734ed",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769239483572}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769239464991}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(T_ISSUES_OCCUR)\n",
    "\n",
    "# Property-level aggregation\n",
    "prop_agg = (\n",
    "    df.groupBy(PROPERTY_COL, SELLER_ID_COL, \"issue\")\n",
    "      .agg(\n",
    "          F.count(\"*\").alias(\"mentions\"),\n",
    "          F.avg(\"issue_sim\").alias(\"avg_issue_sim\"),\n",
    "          F.stddev(\"issue_sim\").alias(\"std_issue_sim\"),\n",
    "          F.expr(\"percentile_approx(issue_sim, 0.5)\").alias(\"median_issue_sim\"),\n",
    "          F.avg(\"neg_prob\").alias(\"avg_neg_prob\"),\n",
    "          F.stddev(\"neg_prob\").alias(\"std_neg_prob\"),\n",
    "          F.expr(\"percentile_approx(neg_prob, 0.5)\").alias(\"median_neg_prob\"),\n",
    "          F.max(\"neg_prob\").alias(\"max_neg_prob\"),\n",
    "      )\n",
    "      .withColumn(\"avg_issue_sim\", F.round(\"avg_issue_sim\", 3))\n",
    "      .withColumn(\"std_issue_sim\", F.round(\"std_issue_sim\", 3))\n",
    "      .withColumn(\"median_issue_sim\", F.round(\"median_issue_sim\", 3))\n",
    "      .withColumn(\"avg_neg_prob\", F.round(\"avg_neg_prob\", 3))\n",
    "      .withColumn(\"std_neg_prob\", F.round(\"std_neg_prob\", 3))\n",
    "      .withColumn(\"median_neg_prob\", F.round(\"median_neg_prob\", 3))\n",
    "      .withColumn(\"max_neg_prob\", F.round(\"max_neg_prob\", 3))\n",
    ")\n",
    "\n",
    "save_table_and_path(prop_agg, T_PROP_ISSUE_AGG)\n",
    "print(\"[OK] Saved:\", T_PROP_ISSUE_AGG, \"| rows:\", spark.table(T_PROP_ISSUE_AGG).count())\n",
    "display(spark.table(T_PROP_ISSUE_AGG).limit(50))\n",
    "\n",
    "# Host-level aggregation\n",
    "host_agg = (\n",
    "    df.groupBy(SELLER_ID_COL, \"issue\")\n",
    "      .agg(\n",
    "          F.count(\"*\").alias(\"mentions\"),\n",
    "          F.countDistinct(PROPERTY_COL).alias(\"n_properties\"),\n",
    "          F.avg(\"issue_sim\").alias(\"avg_issue_sim\"),\n",
    "          F.expr(\"percentile_approx(issue_sim, 0.5)\").alias(\"median_issue_sim\"),\n",
    "          F.avg(\"neg_prob\").alias(\"avg_neg_prob\"),\n",
    "      )\n",
    "      .withColumn(\"avg_issue_sim\", F.round(\"avg_issue_sim\", 3))\n",
    "      .withColumn(\"median_issue_sim\", F.round(\"median_issue_sim\", 3))\n",
    "      .withColumn(\"avg_neg_prob\", F.round(\"avg_neg_prob\", 3))\n",
    "      .orderBy(F.desc(\"mentions\"))\n",
    ")\n",
    "\n",
    "save_table_and_path(host_agg, T_HOST_ISSUE_AGG)\n",
    "print(\"[OK] Saved:\", T_HOST_ISSUE_AGG, \"| rows:\", spark.table(T_HOST_ISSUE_AGG).count())\n",
    "display(spark.table(T_HOST_ISSUE_AGG).limit(50))\n",
    "\n",
    "# Evidence tables\n",
    "K_EVID = 5\n",
    "\n",
    "w_prop = Window.partitionBy(PROPERTY_COL, SELLER_ID_COL, \"issue\") \\\n",
    "               .orderBy(F.desc(\"issue_sim\"), F.desc(\"neg_prob\"))\n",
    "\n",
    "prop_evid = (df.withColumn(\"ev_rank\", F.row_number().over(w_prop))\n",
    "      .select(PROPERTY_COL, SELLER_ID_COL, \"issue\", \"ev_rank\",\n",
    "          \"issue_sim\", \"neg_prob\", \"sentence\", \"sentence_id\",\n",
    "          \"is_top2\",\"top_gap\",\"top1_issue\",\"top2_issue\"))\n",
    "\n",
    "save_table_and_path(prop_evid, T_PROP_ISSUE_EVID)\n",
    "print(\"[OK] Saved:\", T_PROP_ISSUE_EVID, \"| rows:\", spark.table(T_PROP_ISSUE_EVID).count())\n",
    "display(spark.table(T_PROP_ISSUE_EVID).limit(50))\n",
    "\n",
    "w_host = Window.partitionBy(SELLER_ID_COL, \"issue\") \\\n",
    "               .orderBy(F.desc(\"issue_sim\"), F.desc(\"neg_prob\"))\n",
    "\n",
    "host_evid = (df.withColumn(\"ev_rank\", F.row_number().over(w_host))\n",
    "      .select(SELLER_ID_COL, \"issue\", \"ev_rank\",\n",
    "          PROPERTY_COL, \"issue_sim\", \"neg_prob\",\n",
    "          \"sentence\", \"sentence_id\",\n",
    "          \"is_top2\",\"top_gap\",\"top1_issue\",\"top2_issue\"))\n",
    "\n",
    "save_table_and_path(host_evid, T_HOST_ISSUE_EVID)\n",
    "print(\"[OK] Saved:\", T_HOST_ISSUE_EVID, \"| rows:\", spark.table(T_HOST_ISSUE_EVID).count())\n",
    "display(spark.table(T_HOST_ISSUE_EVID).limit(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8704bb75-1e94-49cb-ba2c-ee607aa31fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7 - Global summaries (Top issues, most problematic properties/hosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6255a604-0e41-4eef-9888-da5362acec87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_prop = spark.table(T_PROP_ISSUE_AGG)\n",
    "df_host = spark.table(T_HOST_ISSUE_AGG)\n",
    "\n",
    "print(\"[REPORT] Global top issues (by mentions)\")\n",
    "global_issues = (\n",
    "    df_prop.groupBy(\"issue\")\n",
    "           .agg(\n",
    "               F.sum(\"mentions\").alias(\"mentions\"),\n",
    "               F.countDistinct(PROPERTY_COL).alias(\"n_properties\"),\n",
    "               F.avg(\"avg_neg_prob\").alias(\"avg_neg_prob\"),\n",
    "               F.avg(\"avg_issue_sim\").alias(\"avg_issue_sim\"),\n",
    "           )\n",
    "           .withColumn(\"avg_neg_prob\", F.round(\"avg_neg_prob\", 3))\n",
    "           .withColumn(\"avg_issue_sim\", F.round(\"avg_issue_sim\", 3))\n",
    "           .orderBy(F.desc(\"mentions\"))\n",
    ")\n",
    "display(global_issues.limit(30))\n",
    "\n",
    "print(\"[REPORT] Most problematic properties (high mentions + high neg_prob)\")\n",
    "worst_props = (\n",
    "    df_prop.withColumn(\"red_flag\", F.col(\"mentions\") * F.col(\"avg_neg_prob\") * F.col(\"avg_issue_sim\"))\n",
    "           .orderBy(F.desc(\"red_flag\"))\n",
    ")\n",
    "display(worst_props.limit(50))\n",
    "\n",
    "print(\"[REPORT] Most problematic hosts (high mentions across many properties)\")\n",
    "worst_hosts = (\n",
    "    df_host.withColumn(\"red_flag\", F.col(\"mentions\") * F.col(\"avg_neg_prob\") * F.col(\"avg_issue_sim\"))\n",
    "           .orderBy(F.desc(\"red_flag\"))\n",
    ")\n",
    "display(worst_hosts.limit(50))\n",
    "\n",
    "example = worst_props.select(PROPERTY_COL, SELLER_ID_COL).limit(1).collect()\n",
    "if example:\n",
    "    pid = example[0][PROPERTY_COL]\n",
    "    sid = example[0][SELLER_ID_COL]\n",
    "    print(f\"[REPORT] Example property drilldown: property_id={pid} | seller_id={sid}\")\n",
    "\n",
    "    display(\n",
    "        spark.table(T_PROP_ISSUE_AGG)\n",
    "             .filter((F.col(PROPERTY_COL) == pid) & (F.col(SELLER_ID_COL) == sid))\n",
    "             .orderBy(F.desc(\"mentions\"), F.desc(\"avg_neg_prob\"))\n",
    "    )\n",
    "    display(\n",
    "        spark.table(T_PROP_ISSUE_EVID)\n",
    "             .filter((F.col(PROPERTY_COL) == pid) & (F.col(SELLER_ID_COL) == sid))\n",
    "             .orderBy(\"issue\",\"ev_rank\")\n",
    "    )\n",
    "\n",
    "print(\"[OK] Pipeline complete. All tables saved with RUN_ID:\", RUN_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5addfbd2-2eff-4910-bf41-3c6bb7206ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LLM recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b8c288-c5b8-4de5-a2ae-dc3a096ae16f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================\n",
    "# GOOGLE API KEY (REQUIRED)\n",
    "# =============================\n",
    "# Insert your personal Google Generative AI API key below.\n",
    "# This key is used only to generate live LLM recommendations\n",
    "# and is NOT stored or shared.\n",
    "\n",
    "GOOGLE_API_KEY = \"\"   # <-- paste your API key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b4ab14-74a6-493b-af01-f3ba928e6a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# API config\n",
    "\n",
    "# GOOGLE_API_KEY = None\n",
    "GEMINI_MODEL = \"models/gemini-2.5-pro\"\n",
    "API_SLEEP_SEC = 0.25\n",
    "API_TIMEOUT_SEC = 120\n",
    "\n",
    "# Output tables\n",
    "T_API_TARGETS = f\"airbnb_api_targets_{RUN_ID}\"\n",
    "T_API_RESULTS = f\"airbnb_api_recs_{RUN_ID}\"\n",
    "\n",
    "api_schema = \"property_id STRING, seller_id STRING, ts STRING, request_json STRING, response_json STRING, status STRING, error STRING\"\n",
    "\n",
    "if GOOGLE_API_KEY is None:\n",
    "    print(\"[WARN] GOOGLE_API_KEY is not set. API calls will fail until you load it from secrets/env.\")\n",
    "print(\"[INFO] API targets table:\", T_API_TARGETS)\n",
    "print(\"[INFO] API results table:\", T_API_RESULTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb467022-902b-4e44-93a5-737de9ff07b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "PROPERTY_IDS = [\n",
    "    \"1042005770541410920\",\n",
    "    \"10566180\",\n",
    "    \"11251348\",\n",
    "    \"1166472471627354698\",\n",
    "    \"1259994760695336460\",\n",
    "    \"14410929\",\n",
    "    \"1773055\",\n",
    "]\n",
    "\n",
    "# Evidence table (property-level)\n",
    "EVID_TABLE = T_PROP_ISSUE_EVID\n",
    "\n",
    "# Filter evidence to the chosen properties\n",
    "evid = (\n",
    "    spark.table(EVID_TABLE)\n",
    "         .filter(F.col(PROPERTY_COL).cast(\"string\").isin(PROPERTY_IDS))\n",
    ")\n",
    "\n",
    "# Keep only Top1 evidence (if table contains both top1/top2 info)\n",
    "# If your evidence table has an \"is_top2\" flag, this keeps only top1 rows.\n",
    "if \"is_top2\" in evid.columns:\n",
    "    evid = evid.filter((F.col(\"is_top2\").isNull()) | (F.col(\"is_top2\") == F.lit(False)))\n",
    "\n",
    "# Build targets (one row per property+seller)\n",
    "picked = (\n",
    "    evid.groupBy(PROPERTY_COL, SELLER_ID_COL)\n",
    "        .agg(F.count(\"*\").alias(\"survived_sentences\"))\n",
    "        .orderBy(F.desc(\"survived_sentences\"))\n",
    ")\n",
    "\n",
    "save_table_and_path(picked, T_API_TARGETS)\n",
    "\n",
    "display(picked)\n",
    "print(\"[OK] Targets saved:\", T_API_TARGETS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1dc769-c263-48a1-8a15-604e4fa43352",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def build_payload_for_property(property_id: str, seller_id: str, max_sentences: int = 120):\n",
    "    \"\"\"\n",
    "    Build a compact payload for the LLM (one property):\n",
    "    - Input: negative + issue-tagged sentences for (property_id, seller_id)\n",
    "    - Output: a compact dict with metadata + sentences list\n",
    "    \"\"\"\n",
    "    df = (spark.table(T_ISSUES_OCCUR)\n",
    "             .filter((F.col(PROPERTY_COL) == str(property_id)) & (F.col(SELLER_ID_COL) == str(seller_id)))\n",
    "             .orderBy(F.desc(\"neg_prob\"), F.desc(\"issue_sim\"))\n",
    "             .limit(int(max_sentences))\n",
    "             .select(\n",
    "                 PROPERTY_COL, SELLER_ID_COL,\n",
    "                 \"sentence_id\", \"sentence\",\n",
    "                 \"issue\", \"issue_sim\",\n",
    "                 \"neg_prob\",\n",
    "                 \"is_top2\", \"top_gap\", \"top1_issue\", \"top2_issue\"))\n",
    "\n",
    "    rows = [r.asDict(True) for r in df.collect()]\n",
    "\n",
    "    return {\n",
    "        \"entity_type\": \"property\",\n",
    "        \"property_id\": str(property_id),\n",
    "        \"seller_id\": str(seller_id),\n",
    "        \"sentences\": rows\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87bcce56-5527-4c15-ac96-680eebe181d4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769243026516}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time, requests\n",
    "from datetime import datetime\n",
    "\n",
    "def call_gemini_api(payload: dict, timeout_sec: int = 120):\n",
    "    if not GOOGLE_API_KEY:\n",
    "        return False, 0, None, \"GOOGLE_API_KEY is not set\"\n",
    "\n",
    "    url = f\"https://generativelanguage.googleapis.com/v1beta/{GEMINI_MODEL}:generateContent?key={GOOGLE_API_KEY}\"\n",
    "\n",
    "    prompt_text = (\n",
    "        \"You are an expert hospitality operations consultant.\\n\\n\"\n",
    "        \"Analyze the following Airbnb feedback for ONE property.\\n\"\n",
    "        \"Each item includes: sentence, neg_prob (probability the sentence is negative), and issue category.\\n\\n\"\n",
    "        \"Return a short summary and practical recommendations for the host.\\n\\n\"\n",
    "        \"OUTPUT FORMAT (strict):\\n\"\n",
    "        \"Summary: <2-3 concise sentences describing the main problems and their impact>\\n\"\n",
    "        \"Priority: P0 or P1 (P0 = urgent/critical, P1 = important but not critical)\\n\"\n",
    "        \"Practical recommendations:\\n\"\n",
    "        \"- <actionable recommendation 1>\\n\"\n",
    "        \"- <actionable recommendation 2>\\n\"\n",
    "        \"- <actionable recommendation 3>\\n\"\n",
    "        \"(2-6 bullets total; be specific and implementable, e.g., noise -> door seals / double glazing / thick curtains)\\n\\n\"\n",
    "        \"Do NOT return JSON. Do NOT add extra sections or titles. Do NOT cite sentence_ids.\\n\\n\"\n",
    "        f\"DATA:\\n{json.dumps(payload, ensure_ascii=False, indent=2)}\"\n",
    "    )\n",
    "\n",
    "    body = {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": prompt_text}]}]}\n",
    "    r = requests.post(url, json=body, timeout=timeout_sec)\n",
    "\n",
    "    if 200 <= r.status_code < 300:\n",
    "        return True, r.status_code, r.json(), None\n",
    "    return False, r.status_code, None, (r.text[:2000] if r.text else \"HTTP error\")\n",
    "\n",
    "# Ensure results table exists\n",
    "if not spark.catalog.tableExists(T_API_RESULTS):\n",
    "    spark.createDataFrame([], api_schema).write.mode(\"overwrite\").format(\"delta\").saveAsTable(T_API_RESULTS)\n",
    "    print(\"[OK] Init API results table:\", T_API_RESULTS)\n",
    "\n",
    "targets = spark.table(T_API_TARGETS).collect()\n",
    "print(\"[INFO] API targets:\", [(t[PROPERTY_COL], t[SELLER_ID_COL], t[\"survived_sentences\"]) for t in targets])\n",
    "\n",
    "rows_to_save = []\n",
    "\n",
    "for t in targets:\n",
    "    pid = t[PROPERTY_COL]\n",
    "    sid = t[SELLER_ID_COL]\n",
    "\n",
    "    payload = build_payload_for_property(pid, sid)\n",
    "\n",
    "    try:\n",
    "        ok, code, resp_json, err = call_gemini_api(payload, timeout_sec=int(API_TIMEOUT_SEC))\n",
    "        rows_to_save.append((\n",
    "            str(pid), str(sid),\n",
    "            datetime.utcnow().isoformat(),\n",
    "            json.dumps(payload, ensure_ascii=False),\n",
    "            json.dumps(resp_json, ensure_ascii=False) if resp_json is not None else None,\n",
    "            \"ok\" if ok else f\"http_{code}\",\n",
    "            err\n",
    "        ))\n",
    "        print(f\"[API] property={pid} seller={sid} -> {'OK' if ok else 'ERR'} ({code})\")\n",
    "    except Exception as e:\n",
    "        rows_to_save.append((\n",
    "            str(pid), str(sid),\n",
    "            datetime.utcnow().isoformat(),\n",
    "            json.dumps(payload, ensure_ascii=False),\n",
    "            None,\n",
    "            \"exception\",\n",
    "            repr(e)\n",
    "        ))\n",
    "        print(f\"[API] property={pid} seller={sid} -> EXCEPTION: {e}\")\n",
    "\n",
    "    time.sleep(float(API_SLEEP_SEC))\n",
    "\n",
    "df_out = spark.createDataFrame(rows_to_save, api_schema)\n",
    "\n",
    "# append to metastore table\n",
    "df_out.write.mode(\"append\").format(\"delta\").saveAsTable(T_API_RESULTS)\n",
    "\n",
    "# also append to OUTPUT_BASE path\n",
    "df_out.write.mode(\"append\").format(\"delta\").save(f\"{OUTPUT_BASE}/{T_API_RESULTS}\")\n",
    "\n",
    "display(spark.table(T_API_RESULTS).orderBy(F.desc(\"ts\")))\n",
    "print(\"[OK] Saved:\", T_API_RESULTS, \"| appended_rows:\", df_out.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68e255c1-24f6-48ba-9552-156cbf1b797b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "PROPERTY_IDS = [\n",
    "    \"1042005770541410920\",\n",
    "    \"10566180\",\n",
    "    \"11251348\",\n",
    "    \"1166472471627354698\",\n",
    "    \"1259994760695336460\",\n",
    "    \"14410929\",\n",
    "    \"1773055\",\n",
    "]\n",
    "\n",
    "api_ok = (\n",
    "    spark.table(T_API_RESULTS)\n",
    "         .filter(F.col(\"status\") == \"ok\")\n",
    "         .filter(F.col(\"property_id\").cast(\"string\").isin(PROPERTY_IDS))   # <-- במקום RUN_DATE\n",
    "         .select(\n",
    "             F.col(\"property_id\").cast(\"string\").alias(\"property_id\"),\n",
    "             F.col(\"seller_id\").cast(\"string\").alias(\"seller_id\"),\n",
    "             F.get_json_object(\n",
    "                 \"response_json\",\n",
    "                 \"$.candidates[0].content.parts[0].text\"\n",
    "             ).alias(\"llm_text\"),\n",
    "             \"ts\"\n",
    "         )\n",
    ")\n",
    "\n",
    "agg = (\n",
    "    spark.table(T_PROP_ISSUE_AGG)\n",
    "         .select(\n",
    "             F.col(PROPERTY_COL).cast(\"string\").alias(\"property_id\"),\n",
    "             F.col(SELLER_ID_COL).cast(\"string\").alias(\"seller_id\"),\n",
    "             \"issue\",\n",
    "             \"mentions\",\n",
    "             \"avg_issue_sim\",\n",
    "             \"median_issue_sim\",\n",
    "             \"avg_neg_prob\",\n",
    "             \"median_neg_prob\",\n",
    "             \"max_neg_prob\"\n",
    "         )\n",
    ")\n",
    "\n",
    "api_prop_issue = (\n",
    "    api_ok\n",
    "    .join(agg, on=[\"property_id\", \"seller_id\"], how=\"inner\")\n",
    ")\n",
    "\n",
    "display(api_prop_issue.orderBy(F.desc(\"mentions\")).limit(50))\n",
    "\n",
    "evid = (\n",
    "    spark.table(T_PROP_ISSUE_EVID)\n",
    "         .select(\n",
    "             F.col(PROPERTY_COL).cast(\"string\").alias(\"property_id\"),\n",
    "             F.col(SELLER_ID_COL).cast(\"string\").alias(\"seller_id\"),\n",
    "             \"issue\",\n",
    "             \"sentence_id\",\n",
    "             \"sentence\",\n",
    "             \"neg_prob\",\n",
    "             \"issue_sim\"\n",
    "         )\n",
    ")\n",
    "\n",
    "api_prop_issue_evid = (\n",
    "    api_prop_issue\n",
    "    .join(evid, on=[\"property_id\",\"seller_id\",\"issue\"], how=\"left\")\n",
    ")\n",
    "\n",
    "display(api_prop_issue_evid.limit(50))\n",
    "\n",
    "T_API_PROP_ISSUE = \"airbnb_api_property_issue_ITALY_1\"\n",
    "T_API_PROP_ISSUE_EVID = \"airbnb_api_property_issue_evidence_ITALY_1\"\n",
    "\n",
    "save_table_and_path(api_prop_issue, T_API_PROP_ISSUE)\n",
    "save_table_and_path(api_prop_issue_evid, T_API_PROP_ISSUE_EVID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba294b4d-d306-441c-94d6-b3b495ef6f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Property+Issue aggregate table (mentions + stats)\n",
    "agg_all = (\n",
    "    spark.table(T_PROP_ISSUE_AGG)\n",
    "         .select(\n",
    "             F.col(PROPERTY_COL).cast(\"string\").alias(\"property_id\"),\n",
    "             F.col(SELLER_ID_COL).cast(\"string\").alias(\"seller_id\"),\n",
    "             \"issue\",\n",
    "             \"mentions\",\n",
    "             \"avg_issue_sim\",\n",
    "             \"median_issue_sim\",\n",
    "             \"avg_neg_prob\",\n",
    "             \"median_neg_prob\",\n",
    "             \"max_neg_prob\"\n",
    "         )\n",
    ")\n",
    "\n",
    "# Evidence table (all sentences per property+issue)\n",
    "evid_all = (\n",
    "    spark.table(T_PROP_ISSUE_EVID)\n",
    "         .select(\n",
    "             F.col(PROPERTY_COL).cast(\"string\").alias(\"property_id\"),\n",
    "             F.col(SELLER_ID_COL).cast(\"string\").alias(\"seller_id\"),\n",
    "             \"issue\",\n",
    "             \"sentence_id\",\n",
    "             \"sentence\",\n",
    "             \"neg_prob\",\n",
    "             \"issue_sim\",\n",
    "             *([c for c in [\"ev_rank\", \"sent_shard\", \"ts\"] if c in spark.table(T_PROP_ISSUE_EVID).columns])\n",
    "         )\n",
    ")\n",
    "\n",
    "# Full normalized table: one row per sentence evidence, with agg stats duplicated\n",
    "prop_issue_sentence_full = (\n",
    "    agg_all\n",
    "    .join(evid_all, on=[\"property_id\", \"seller_id\", \"issue\"], how=\"left\")\n",
    ")\n",
    "\n",
    "display(prop_issue_sentence_full.orderBy(F.desc(\"mentions\")).limit(50))\n",
    "\n",
    "# Save\n",
    "T_PROP_ISSUE_SENT_FULL = f\"airbnb_property_issue_sentence_full_{RUN_ID}\"\n",
    "save_table_and_path(prop_issue_sentence_full, T_PROP_ISSUE_SENT_FULL)\n",
    "print(\"Saved:\", T_PROP_ISSUE_SENT_FULL)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6141037208928322,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Airbnb_Reviews_Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}