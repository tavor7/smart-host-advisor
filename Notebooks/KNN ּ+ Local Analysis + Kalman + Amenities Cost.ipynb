{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8178a26c-a5e8-4803-b1e6-be10383f5742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Loading datasets into a dataframe\n",
    "\n",
    "last update: 2025-12-24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c71d2f-7062-40f0-850e-85c42f5b1197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"lab94290\"  \n",
    "container = \"airbnb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5877cd9-f2d5-4cd4-b72f-4586a0e6aa86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Reading airbnb data from Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8f8b87-a2ce-479e-af65-8d702b522fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sas_token=\"sp=rle&st=2025-12-24T17:37:04Z&se=2026-02-28T01:52:04Z&spr=https&sv=2024-11-04&sr=c&sig=a0lx%2BS6PuS%2FvJ9Tbt4NKdCJHLE9d1Y1D6vpE1WKFQtk%3D\"\n",
    "sas_token = sas_token.lstrip('?')\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290cef33-116a-4dd1-8f7d-9e1b541017b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "DataFrame[name: string, price: string, image: string, description: string, category: string, availability: string, discount: string, reviews: string, ratings: string, seller_info: string, breadcrumbs: string, location: string, lat: string, long: string, guests: string, pets_allowed: string, description_items: string, category_rating: string, house_rules: string, details: string, highlights: string, arrangement_details: string, amenities: string, images: string, available_dates: string, url: string, final_url: string, listing_title: string, property_id: string, listing_name: string, location_details: string, description_by_sections: string, description_html: string, location_details_html: string, is_supperhost: string, host_number_of_reviews: string, host_rating: string, hosts_year: string, host_response_rate: string, is_guest_favorite: string, travel_details: string, pricing_details: string, total_price: string, currency: string, cancellation_policy: string, property_number_of_reviews: string, country: string, postcode_map_url: string, host_image: string, host_details: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- price: string (nullable = true)\n |-- image: string (nullable = true)\n |-- description: string (nullable = true)\n |-- category: string (nullable = true)\n |-- availability: string (nullable = true)\n |-- discount: string (nullable = true)\n |-- reviews: string (nullable = true)\n |-- ratings: string (nullable = true)\n |-- seller_info: string (nullable = true)\n |-- breadcrumbs: string (nullable = true)\n |-- location: string (nullable = true)\n |-- lat: string (nullable = true)\n |-- long: string (nullable = true)\n |-- guests: string (nullable = true)\n |-- pets_allowed: string (nullable = true)\n |-- description_items: string (nullable = true)\n |-- category_rating: string (nullable = true)\n |-- house_rules: string (nullable = true)\n |-- details: string (nullable = true)\n |-- highlights: string (nullable = true)\n |-- arrangement_details: string (nullable = true)\n |-- amenities: string (nullable = true)\n |-- images: string (nullable = true)\n |-- available_dates: string (nullable = true)\n |-- url: string (nullable = true)\n |-- final_url: string (nullable = true)\n |-- listing_title: string (nullable = true)\n |-- property_id: string (nullable = true)\n |-- listing_name: string (nullable = true)\n |-- location_details: string (nullable = true)\n |-- description_by_sections: string (nullable = true)\n |-- description_html: string (nullable = true)\n |-- location_details_html: string (nullable = true)\n |-- is_supperhost: string (nullable = true)\n |-- host_number_of_reviews: string (nullable = true)\n |-- host_rating: string (nullable = true)\n |-- hosts_year: string (nullable = true)\n |-- host_response_rate: string (nullable = true)\n |-- is_guest_favorite: string (nullable = true)\n |-- travel_details: string (nullable = true)\n |-- pricing_details: string (nullable = true)\n |-- total_price: string (nullable = true)\n |-- currency: string (nullable = true)\n |-- cancellation_policy: string (nullable = true)\n |-- property_number_of_reviews: string (nullable = true)\n |-- country: string (nullable = true)\n |-- postcode_map_url: string (nullable = true)\n |-- host_image: string (nullable = true)\n |-- host_details: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/airbnb_1_12_parquet\"\n",
    "\n",
    "airbnb = spark.read.parquet(path)\n",
    "display(airbnb.limit(50))\n",
    "airbnb.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0859a9b6-a404-46d9-af34-58372a86a62d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65293e74-d5c9-456e-87c4-0def7eb1148e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "\n",
    "# Similarity\n",
    "SIMILARITY_FEATURES_PRICE  = [\"bedrooms\", \"beds\", \"bathrooms\", \"guests\", \"lat\", \"long\"]\n",
    "SIMILARITY_FEATURES_RATING = [\"bedrooms\", \"beds\", \"bathrooms\", \"guests\", \"lat\", \"long\", \"price\"]\n",
    "\n",
    "FEATURE_WEIGHTS = {\n",
    "    \"bedrooms\": 1,\n",
    "    \"beds\": 1,\n",
    "    \"bathrooms\": 1,\n",
    "    \"guests\": 1,\n",
    "    \"lat\": 3,\n",
    "    \"long\": 3,\n",
    "    \"price\": 1,   \n",
    "}\n",
    "\n",
    "N_NEIGHBORS = 20\n",
    "\n",
    "# Amenity logic\n",
    "MUST_HAVE_THRESHOLD = 0.9\n",
    "MIN_SUPPORT = 3\n",
    "\n",
    "# Default (for future combine)\n",
    "PRICE_WEIGHT = 0.5\n",
    "RATING_WEIGHT = 0.5\n",
    "\n",
    "OUTPUT_BASE = \"dbfs:/FileStore/airbnb\"\n",
    "\n",
    "GRID_SIZE = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "888229c6-1e1a-45f1-8f3c-5f12f4650f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Local analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e488163d-093a-4be4-8a42-4afac1c17d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# IMPORTS\n",
    "# =========================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, ArrayType, StringType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "import ast, re\n",
    "\n",
    "\n",
    "# =========================\n",
    "# STRUCTURAL FEATURES \n",
    "# =========================\n",
    "def extract_structural_features_spark(df):\n",
    "\n",
    "    def extract_value(items, keyword):\n",
    "        if not isinstance(items, list):\n",
    "            return None\n",
    "        for item in items:\n",
    "            if not isinstance(item, str):\n",
    "                continue\n",
    "            lowered = item.lower()\n",
    "            if keyword in lowered:\n",
    "                m = re.search(r\"(\\d+)\", lowered)\n",
    "                if m:\n",
    "                    return int(m.group(1))\n",
    "        return None\n",
    "\n",
    "    extract_udf = F.udf(lambda x, k: extract_value(x, k), IntegerType())\n",
    "\n",
    "    # description_items\n",
    "    if \"description_items\" in df.columns:\n",
    "        df = df.withColumn(\"bedrooms\", extract_udf(F.col(\"description_items\"), F.lit(\"bedroom\")))\n",
    "        df = df.withColumn(\"beds\", extract_udf(F.col(\"description_items\"), F.lit(\" bed\")))\n",
    "        df = df.withColumn(\"bathrooms\", extract_udf(F.col(\"description_items\"), F.lit(\"bath\")))\n",
    "\n",
    "    # details (string list -> list)\n",
    "    if \"details\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"details_list\",\n",
    "            F.when(\n",
    "                F.col(\"details\").isNotNull() & F.col(\"details\").startswith(\"[\"),\n",
    "                F.from_json(F.col(\"details\"), \"array<string>\")\n",
    "            ).otherwise(F.lit(None))\n",
    "        )\n",
    "\n",
    "        df = df.withColumn(\"guests\", extract_udf(F.col(\"details_list\"), F.lit(\"guest\")))\n",
    "\n",
    "        # Fill missing structural (coalesce)\n",
    "        df = df.withColumn(\"bedrooms\", F.coalesce(F.col(\"bedrooms\"), extract_udf(F.col(\"details_list\"), F.lit(\"bedroom\"))))\n",
    "        df = df.withColumn(\"beds\", F.coalesce(F.col(\"beds\"), extract_udf(F.col(\"details_list\"), F.lit(\" bed\"))))\n",
    "        df = df.withColumn(\"bathrooms\", F.coalesce(F.col(\"bathrooms\"), extract_udf(F.col(\"details_list\"), F.lit(\"bath\"))))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c471d610-c7a4-4ac3-b416-a1d682f891ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# PREP DF\n",
    "# =========================\n",
    "df = airbnb.drop(\"price\")\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"price\", F.get_json_object(F.col(\"pricing_details\"), \"$.price_per_night\").cast(\"double\"))\n",
    "    .withColumn(\"ratings\", F.col(\"ratings\").cast(\"double\"))\n",
    "    .withColumn(\"property_id\", F.col(\"property_id\").cast(\"string\"))\n",
    "    .withColumn(\"country\", F.trim(F.element_at(F.split(F.col(\"location\"), \",\"), -1)))\n",
    "    .withColumn(\"lat_bin\", F.floor(F.col(\"lat\") / F.lit(GRID_SIZE)))\n",
    "    .withColumn(\"lon_bin\", F.floor(F.col(\"long\") / F.lit(GRID_SIZE)))\n",
    ")\n",
    "\n",
    "df = extract_structural_features_spark(df)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# DISTANCE\n",
    "# =========================\n",
    "def weighted_distance_expr(prefix_a, prefix_b, features):\n",
    "    exprs = []\n",
    "    for f in features:\n",
    "        w = FEATURE_WEIGHTS.get(f, 1.0)\n",
    "\n",
    "        # NULL-safe: missing feature contributes 0\n",
    "        diff = F.when(\n",
    "            F.col(f\"{prefix_a}.{f}\").isNull() | F.col(f\"{prefix_b}.{f}\").isNull(),\n",
    "            F.lit(0.0)\n",
    "        ).otherwise(F.col(f\"{prefix_a}.{f}\") - F.col(f\"{prefix_b}.{f}\"))\n",
    "\n",
    "        exprs.append(F.lit(float(w)) * diff ** 2)\n",
    "\n",
    "    return F.sqrt(sum(exprs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4fcd202-4dc2-414d-8011-c804257dbe69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# KNN\n",
    "# =========================\n",
    "def compute_knn(df, mode=\"price\"):\n",
    "    \"\"\"\n",
    "    mode:\n",
    "      - \"price\"  : distance uses structural + location\n",
    "      - \"rating\" : distance uses structural + location + price\n",
    "    \"\"\"\n",
    "    a = df.alias(\"a\")\n",
    "\n",
    "    # neighbors must have price; for rating-mode also must have ratings\n",
    "    b = df.filter(F.col(\"price\").isNotNull())\n",
    "    if mode == \"rating\":\n",
    "        b = b.filter(F.col(\"ratings\").isNotNull())\n",
    "        features = SIMILARITY_FEATURES_RATING\n",
    "    else:\n",
    "        features = SIMILARITY_FEATURES_PRICE\n",
    "\n",
    "    b = b.alias(\"b\")\n",
    "\n",
    "    joined = (\n",
    "        a.join(\n",
    "            b,\n",
    "            (F.col(\"a.property_id\") != F.col(\"b.property_id\")) &\n",
    "            (F.col(\"a.country\") == F.col(\"b.country\")) &\n",
    "            (F.col(\"a.lat_bin\") == F.col(\"b.lat_bin\")) &\n",
    "            (F.col(\"a.lon_bin\") == F.col(\"b.lon_bin\"))\n",
    "        )\n",
    "        .withColumn(\"distance\", weighted_distance_expr(\"a\", \"b\", features))\n",
    "        .withColumn(\"similarity\", 1 / (1 + F.col(\"distance\")))\n",
    "    )\n",
    "\n",
    "    # window rank\n",
    "    w = Window.partitionBy(\"a.property_id\").orderBy(\"distance\")\n",
    "\n",
    "    knn = (\n",
    "        joined\n",
    "        .withColumn(\"rank\", F.row_number().over(w))\n",
    "        .filter(F.col(\"rank\") <= N_NEIGHBORS)\n",
    "        .select(\n",
    "            F.col(\"a.property_id\").alias(\"target_id\"),\n",
    "            F.col(\"b.property_id\").alias(\"neighbor_id\"),\n",
    "            \"distance\",\n",
    "            \"similarity\",\n",
    "\n",
    "            # Baseline target fields (needed for gains / comparisons)\n",
    "            F.col(\"a.price\").alias(\"target_price\"),\n",
    "            F.col(\"a.ratings\").alias(\"target_ratings\"),\n",
    "            F.col(\"a.amenities\").alias(\"target_amenities\"),\n",
    "\n",
    "            # Neighbor fields\n",
    "            F.col(\"b.price\").alias(\"price\"),\n",
    "            F.col(\"b.ratings\").alias(\"ratings\"),\n",
    "            F.col(\"b.amenities\").alias(\"amenities\"),\n",
    "            F.col(\"b.house_rules\").alias(\"house_rules\"),\n",
    "            F.col(\"b.pets_allowed\").alias(\"pets_allowed\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return knn\n",
    "\n",
    "\n",
    "# =========================\n",
    "# AMENITY EXTRACTION + CHECKIN/CHECKOUT\n",
    "# =========================\n",
    "def extract_amenities(val):\n",
    "    if val is None:\n",
    "        return []\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            val = ast.literal_eval(val)\n",
    "        except:\n",
    "            return []\n",
    "    if not isinstance(val, list):\n",
    "        return []\n",
    "    names = []\n",
    "    for g in val:\n",
    "        if isinstance(g, dict):\n",
    "            group = g.get(\"group_name\", \"\").lower()\n",
    "            if \"not included\" in group:\n",
    "                continue \n",
    "            for it in g.get(\"items\", []):\n",
    "                n = str(it.get(\"name\", \"\")).lower().strip()\n",
    "                if n:\n",
    "                    names.append(n)\n",
    "    return names\n",
    "\n",
    "extract_amenities_udf = F.udf(extract_amenities, ArrayType(StringType()))\n",
    "\n",
    "def extract_hour(text, mode=\"in\"):\n",
    "    if not isinstance(text, str):\n",
    "        return None\n",
    "    t = text.lower()\n",
    "    if \"flexible\" in t:\n",
    "        return 8 if mode == \"in\" else 15\n",
    "    m = re.search(r\"(\\d{1,2})\", t)\n",
    "    if not m:\n",
    "        return None\n",
    "    h = int(m.group(1))\n",
    "    if \"pm\" in t and h < 12:\n",
    "        h += 12\n",
    "    if \"am\" in t and h == 12:\n",
    "        h = 0\n",
    "    return max(0, min(22, (h // 2) * 2))\n",
    "\n",
    "checkin_udf = F.udf(lambda x: extract_hour(x, \"in\"), IntegerType())\n",
    "checkout_udf = F.udf(lambda x: extract_hour(x, \"out\"), IntegerType())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03a355b7-5073-46a6-92ba-cb52868afbca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "def is_empty_df(d):\n",
    "    return (d is None) or d.rdd.isEmpty()\n",
    "\n",
    "def build_amenity_reports(neighbors, listing_amenities_flat, mode):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - recs_df: amenities to recommend (freq>=MIN_SUPPORT), score per mode\n",
    "      - popular_df: MUST_HAVE amenities (percentage>=MUST_HAVE_THRESHOLD), show ONLY averages\n",
    "    \"\"\"\n",
    "    \n",
    "    if is_empty_df(neighbors):\n",
    "        rec_schema = (\n",
    "            \"target_id string, amenity string, freq long, total long, percentage double, \"\n",
    "            \"avg_price_with double, avg_rating_with double, score double, category string\"\n",
    "        )\n",
    "        pop_schema = (\n",
    "            \"target_id string, amenity string, freq long, total long, percentage double, \"\n",
    "            \"avg_price_with double, avg_rating_with double, category string\"\n",
    "        )\n",
    "        return (\n",
    "            spark.createDataFrame([], rec_schema),\n",
    "            spark.createDataFrame([], pop_schema),\n",
    "        )\n",
    "\n",
    "    # Parse TARGET amenities (before join - efficient!)\n",
    "    neighbors = neighbors.withColumn(\n",
    "        \"target_amenity_list\",\n",
    "        extract_amenities_udf(\"target_amenities\")\n",
    "    )\n",
    "\n",
    "    # Rename columns to avoid ambiguity\n",
    "    listing_amenities_flat = (\n",
    "        listing_amenities_flat\n",
    "        .withColumnRenamed(\"price\", \"neighbor_price\")\n",
    "        .withColumnRenamed(\"ratings\", \"neighbor_ratings\")\n",
    "    )\n",
    "\n",
    "    # Join neighbors with flat amenities\n",
    "    joined = (\n",
    "        neighbors\n",
    "        .join(\n",
    "            broadcast(listing_amenities_flat),\n",
    "            neighbors.neighbor_id == listing_amenities_flat.property_id,\n",
    "            \"left\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Remove duplicates BEFORE groupBy, filter out NULL amenities\n",
    "    joined_distinct = (\n",
    "        joined\n",
    "        .filter(F.col(\"amenity\").isNotNull())  # ← סינון NULL!\n",
    "        .select(\n",
    "            \"target_id\", \"amenity\", \"neighbor_id\",\n",
    "            \"neighbor_price\", \"neighbor_ratings\",\n",
    "            \"target_price\", \"target_ratings\", \"target_amenity_list\"\n",
    "        )\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # Total neighbors per target - from joined_distinct!\n",
    "    total = (\n",
    "        joined_distinct\n",
    "        .select(\"target_id\", \"neighbor_id\")\n",
    "        .distinct()\n",
    "        .groupBy(\"target_id\")\n",
    "        .count()\n",
    "        .withColumnRenamed(\"count\", \"total\")\n",
    "    )\n",
    "\n",
    "    # Stats per amenity\n",
    "    stats = (\n",
    "        joined_distinct\n",
    "        .groupBy(\"target_id\", \"amenity\")\n",
    "        .agg(\n",
    "            # Count distinct neighbors WITH this amenity\n",
    "            F.countDistinct(\"neighbor_id\").alias(\"freq\"),\n",
    "            \n",
    "            # Averages over neighbors WITH the amenity\n",
    "            F.avg(\"neighbor_price\").alias(\"avg_price_with\"),\n",
    "            \n",
    "            # Ratings: ignore 0, NULL stays NULL\n",
    "            F.avg(\n",
    "                F.when(F.col(\"neighbor_ratings\") > 0, F.col(\"neighbor_ratings\"))\n",
    "            ).alias(\"avg_rating_with\"),\n",
    "            \n",
    "            # Stable target baselines\n",
    "            F.first(\"target_price\", ignorenulls=True).alias(\"target_price\"),\n",
    "            F.first(\"target_ratings\", ignorenulls=True).alias(\"target_ratings\"),\n",
    "            F.first(\"target_amenity_list\", ignorenulls=True).alias(\"target_amenity_list\"),\n",
    "        )\n",
    "        .join(total, \"target_id\", \"left\")\n",
    "        .withColumn(\"percentage\", F.col(\"freq\") / F.col(\"total\"))\n",
    "        .withColumn(\n",
    "            \"category\",\n",
    "            F.when(F.col(\"percentage\") >= MUST_HAVE_THRESHOLD, F.lit(\"must_have\"))\n",
    "             .when(F.col(\"freq\") < MIN_SUPPORT, F.lit(\"uncertain\"))\n",
    "             .otherwise(F.lit(\"value\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # POPULAR / MUST_HAVE\n",
    "    popular_df = (\n",
    "        stats\n",
    "        .filter(F.col(\"percentage\") >= MUST_HAVE_THRESHOLD)\n",
    "        .filter(~F.array_contains(F.col(\"target_amenity_list\"), F.col(\"amenity\")))\n",
    "        .select(\n",
    "            \"target_id\", \"amenity\", \"freq\", \"total\", \"percentage\",\n",
    "            \"avg_price_with\", \"avg_rating_with\",\n",
    "            \"category\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # RECOMMENDATIONS\n",
    "    recs_df = (\n",
    "        stats\n",
    "        .filter(F.col(\"freq\") >= MIN_SUPPORT)\n",
    "        .filter(~F.array_contains(F.col(\"target_amenity_list\"), F.col(\"amenity\")))\n",
    "    )\n",
    "\n",
    "    if mode == \"price\":\n",
    "        recs_df = recs_df.withColumn(\n",
    "            \"score\",\n",
    "            F.col(\"avg_price_with\") - F.col(\"target_price\")\n",
    "        )\n",
    "    elif mode == \"rating\":\n",
    "        recs_df = recs_df.withColumn(\n",
    "            \"score\",\n",
    "            F.col(\"avg_rating_with\") - F.col(\"target_ratings\")\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'price' or 'rating'\")\n",
    "\n",
    "    recs_df = recs_df.select(\n",
    "        \"target_id\", \"amenity\", \"freq\", \"total\", \"percentage\",\n",
    "        \"avg_price_with\", \"avg_rating_with\",\n",
    "        \"score\",\n",
    "        \"category\"\n",
    "    )\n",
    "\n",
    "    return recs_df, popular_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73856319-bec5-40d6-aa7b-e48e194c3a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# RUN + SAVE\n",
    "# =========================\n",
    "neighbors_price = compute_knn(df, mode=\"price\").repartition(\"target_id\")\n",
    "neighbors_price.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/neighbors_price\")\n",
    "\n",
    "neighbors_rating = compute_knn(df, mode=\"rating\").repartition(\"target_id\")\n",
    "neighbors_rating.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/neighbors_rating\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6801a4b2-3757-47bf-b5f9-620d7dcb5f3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def build_listing_amenities_flat(listings_df):\n",
    "    df = listings_df.withColumn(\n",
    "        \"amenity_list\",\n",
    "        extract_amenities_udf(\"amenities\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\"checkin\", checkin_udf(\"house_rules\"))\n",
    "    df = df.withColumn(\"checkout\", checkout_udf(\"house_rules\"))\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"amenity_list\",\n",
    "        F.when(\n",
    "            F.col(\"checkin\").isNotNull(),\n",
    "            F.concat(\n",
    "                F.col(\"amenity_list\"),\n",
    "                F.array(F.format_string(\n",
    "                    \"checkin_%02d:00-%02d:00\",\n",
    "                    F.col(\"checkin\"), F.col(\"checkin\") + 2\n",
    "                ))\n",
    "            )\n",
    "        ).otherwise(F.col(\"amenity_list\"))\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"amenity_list\",\n",
    "        F.when(\n",
    "            F.col(\"checkout\").isNotNull(),\n",
    "            F.concat(\n",
    "                F.col(\"amenity_list\"),\n",
    "                F.array(F.format_string(\n",
    "                    \"checkout_%02d:00-%02d:00\",\n",
    "                    F.col(\"checkout\"), F.col(\"checkout\") + 2\n",
    "                ))\n",
    "            )\n",
    "        ).otherwise(F.col(\"amenity_list\"))\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"amenity_list\",\n",
    "        F.when(\n",
    "            F.col(\"pets_allowed\") == True,\n",
    "            F.concat(F.col(\"amenity_list\"), F.array(F.lit(\"pets_allowed\")))\n",
    "        ).otherwise(F.col(\"amenity_list\"))\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"amenity_list\",\n",
    "        F.when(F.size(\"amenity_list\") > 0, F.col(\"amenity_list\"))\n",
    "         .otherwise(F.array(F.lit(\"__NO_AMENITY__\")))\n",
    "    )\n",
    "\n",
    "    listing_amenities_flat = (\n",
    "        df\n",
    "        .withColumn(\"amenity\", F.explode(\"amenity_list\"))\n",
    "        .filter(F.col(\"amenity\") != \"__NO_AMENITY__\")\n",
    "        .select(\n",
    "            F.col(\"property_id\"),\n",
    "            \"amenity\",\n",
    "            \"price\",\n",
    "            \"ratings\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return listing_amenities_flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8548f32d-4229-413e-877e-291211794d99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "listing_amenities_flat = build_listing_amenities_flat(df)\n",
    "\n",
    "listing_amenities_flat.write.mode(\"overwrite\").parquet(\n",
    "    f\"{OUTPUT_BASE}/listing_amenities_flat\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d323900-51ca-4ae0-8227-4bd8902dfd11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "neighbors_price = spark.read.parquet(f\"{OUTPUT_BASE}/neighbors_price\")\n",
    "\n",
    "recs_price, popular_price = build_amenity_reports(\n",
    "    neighbors=neighbors_price,\n",
    "    listing_amenities_flat=listing_amenities_flat,\n",
    "    mode=\"price\"\n",
    ")\n",
    "\n",
    "recs_price.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/recs_price_final\")\n",
    "popular_price.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/popular_price_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33aeb5bb-32e2-4568-bc9f-e4012fdb0c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "neighbors_rating = spark.read.parquet(f\"{OUTPUT_BASE}/neighbors_rating\")\n",
    "\n",
    "recs_rating, popular_rating = build_amenity_reports(\n",
    "    neighbors=neighbors_rating,\n",
    "    listing_amenities_flat=listing_amenities_flat,\n",
    "    mode=\"rating\"\n",
    ")\n",
    "\n",
    "recs_rating.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/recs_rating_final\")\n",
    "popular_rating.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/popular_rating_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030c5c32-cb1d-44c7-9866-18afc6331638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_BASE = \"dbfs:/FileStore/airbnb\"\n",
    "recs_rating = spark.read.parquet(f\"{OUTPUT_BASE}/recs_rating_final\")\n",
    "popular_rating = spark.read.parquet(f\"{OUTPUT_BASE}/popular_rating_final\")\n",
    "\n",
    "recs_price = spark.read.parquet(f\"{OUTPUT_BASE}/recs_price_final\")\n",
    "popular_price = spark.read.parquet(f\"{OUTPUT_BASE}/popular_price_final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9adb20-1e1c-42d3-ad66-e0034f16278b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"/mnt/airbnb_outputs/listings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae9159f-3bab-4ace-8dbb-023709a1c6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 Spark session started\n\n\uD83D\uDCB0 START PRICE PIPELINE\n\n==============================\n\uD83D\uDD35 START build_effects_with_variance | mode=price\n==============================\n\uD83D\uDCE5 Loading parquet files...\n\uD83E\uDDE9 Building amenity sets per property...\n\uD83D\uDCB0 Preparing property values (price / rating)...\n\uD83C\uDFAF Extracting relevant (target_id, amenity) pairs...\n\uD83C\uDF0D Building universe (neighbors × relevant amenities)...\n\uD83D\uDCD0 Selecting value column...\n   using neighbor_price (>0)\n➕ Computing WITH stats...\n➖ Computing WITHOUT stats...\n\uD83D\uDCCA Computing variance of effect...\n\uD83E\uDDE0 Enriching recs with variance...\n✅ DONE build_effects_with_variance | mode=price\n\uD83D\uDCBE Writing price output...\n✅ Price output written\n\n⭐ START RATING PIPELINE\n\n==============================\n\uD83D\uDD35 START build_effects_with_variance | mode=rating\n==============================\n\uD83D\uDCE5 Loading parquet files...\n\uD83E\uDDE9 Building amenity sets per property...\n\uD83D\uDCB0 Preparing property values (price / rating)...\n\uD83C\uDFAF Extracting relevant (target_id, amenity) pairs...\n\uD83C\uDF0D Building universe (neighbors × relevant amenities)...\n\uD83E\uDDF9 [RATING FIX] Filtering invalid neighbor ratings (NULL or <=0)...\n\uD83E\uDDF9 [RATING FIX] Filtering targets with invalid ratings (NULL or <=0)...\n\uD83D\uDCD0 Selecting value column...\n   using neighbor_ratings (>0, already filtered)\n➕ Computing WITH stats...\n➖ Computing WITHOUT stats...\n\uD83D\uDCCA Computing variance of effect...\n\uD83E\uDDE0 Enriching recs with variance...\n\uD83E\uDDF9 [RATING FIX] Dropping recs where target rating is NULL/0 (safe)...\n✅ DONE build_effects_with_variance | mode=rating\n\uD83D\uDCBE Writing rating output...\n✅ Rating output written\n\uD83D\uDED1 Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "OUTPUT_BASE = \"dbfs:/FileStore/airbnb\"\n",
    "NEIGHBORS_PRICE_PATH = f\"{OUTPUT_BASE}/neighbors_price\"\n",
    "NEIGHBORS_RATING_PATH = f\"{OUTPUT_BASE}/neighbors_rating\"\n",
    "LISTING_AMENITIES_FLAT_PATH = f\"{OUTPUT_BASE}/listing_amenities_flat\"\n",
    "RECS_PRICE_PATH = f\"{OUTPUT_BASE}/recs_price_final\"\n",
    "RECS_RATING_PATH = f\"{OUTPUT_BASE}/recs_rating_final\"\n",
    "\n",
    "OUT_PRICE_PATH = f\"{OUTPUT_BASE}/amenity_price_effects.parquet\"\n",
    "OUT_RATING_PATH = f\"{OUTPUT_BASE}/amenity_rating_effects.parquet\"\n",
    "\n",
    "VAR_FALLBACK = 1e6 \n",
    "\n",
    "print(\"\uD83D\uDE80 Spark session started\")\n",
    "\n",
    "# ======================================================\n",
    "# CORE FUNCTION\n",
    "# ======================================================\n",
    "\n",
    "def build_effects_with_variance(\n",
    "    neighbors_path,\n",
    "    recs_path,\n",
    "    amenities_path,\n",
    "    mode=\"price\"  # \"price\" | \"rating\"\n",
    "):\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"\uD83D\uDD35 START build_effects_with_variance | mode={mode}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    print(\"\uD83D\uDCE5 Loading parquet files...\")\n",
    "    neighbors = spark.read.parquet(neighbors_path)\n",
    "    recs = spark.read.parquet(recs_path)\n",
    "    amenities = spark.read.parquet(amenities_path)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # amenity set per property\n",
    "    # --------------------------------------------------\n",
    "    print(\"\uD83E\uDDE9 Building amenity sets per property...\")\n",
    "    amenity_sets = (\n",
    "        amenities\n",
    "        .groupBy(\"property_id\")\n",
    "        .agg(F.collect_set(\"amenity\").alias(\"amenity_set\"))\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # property values\n",
    "    # --------------------------------------------------\n",
    "    print(\"\uD83D\uDCB0 Preparing property values (price / rating)...\")\n",
    "    property_values = (\n",
    "        amenities\n",
    "        .select(\"property_id\", \"price\", \"ratings\")\n",
    "        .distinct()\n",
    "        .withColumnRenamed(\"price\", \"neighbor_price\")\n",
    "        .withColumnRenamed(\"ratings\", \"neighbor_ratings\")\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # relevant pairs\n",
    "    # --------------------------------------------------\n",
    "    print(\"\uD83C\uDFAF Extracting relevant (target_id, amenity) pairs...\")\n",
    "    relevant_pairs = recs.select(\"target_id\", \"amenity\").distinct()\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # universe\n",
    "    # --------------------------------------------------\n",
    "    print(\"\uD83C\uDF0D Building universe (neighbors × relevant amenities)...\")\n",
    "    universe = (\n",
    "        neighbors\n",
    "        .join(amenity_sets, neighbors.neighbor_id == amenity_sets.property_id, \"left\")\n",
    "        .join(property_values, neighbors.neighbor_id == property_values.property_id, \"left\")\n",
    "        .join(relevant_pairs, on=\"target_id\", how=\"inner\")\n",
    "        .withColumn(\n",
    "            \"has_amenity\",\n",
    "            F.coalesce(\n",
    "                F.array_contains(F.col(\"amenity_set\"), F.col(\"amenity\")),\n",
    "                F.lit(False)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ======================================================\n",
    "    # ✅ FIX FOR RATING 0/NULL \n",
    "    # ======================================================\n",
    "    if mode == \"rating\":\n",
    "        print(\"\uD83E\uDDF9 [RATING FIX] Filtering invalid neighbor ratings (NULL or <=0)...\")\n",
    "        universe = universe.filter(\n",
    "            F.col(\"neighbor_ratings\").isNotNull() & (F.col(\"neighbor_ratings\") > 0)\n",
    "        )\n",
    "\n",
    "        print(\"\uD83E\uDDF9 [RATING FIX] Filtering targets with invalid ratings (NULL or <=0)...\")\n",
    "        target_ratings = (\n",
    "            amenities\n",
    "            .select(\n",
    "                F.col(\"property_id\").alias(\"target_id\"),\n",
    "                F.col(\"ratings\").alias(\"target_ratings\")\n",
    "            )\n",
    "            .dropDuplicates([\"target_id\"])\n",
    "        )\n",
    "\n",
    "        target_ratings2 = target_ratings.withColumnRenamed(\"target_ratings\", \"target_ratings_base\")\n",
    "\n",
    "        universe = (\n",
    "            universe\n",
    "            .join(target_ratings2, on=\"target_id\", how=\"left\")\n",
    "            .filter(F.col(\"target_ratings_base\").isNotNull() & (F.col(\"target_ratings_base\") > 0))\n",
    "            .drop(\"target_ratings_base\")\n",
    "        )\n",
    "\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # value column\n",
    "    # --------------------------------------------------\n",
    "    print(\"\uD83D\uDCD0 Selecting value column...\")\n",
    "    if mode == \"price\":\n",
    "        universe = universe.filter(\n",
    "        F.col(\"neighbor_price\").isNotNull() & (F.col(\"neighbor_price\") > 0)\n",
    "        )\n",
    "        value_col = F.log(F.col(\"neighbor_price\"))\n",
    "        print(\"   using neighbor_price (>0)\")\n",
    "    else:\n",
    "        # now neighbor_ratings already valid (no need for when)\n",
    "        value_col = F.col(\"neighbor_ratings\")\n",
    "        print(\"   using neighbor_ratings (>0, already filtered)\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # WITH stats\n",
    "    # --------------------------------------------------\n",
    "    print(\"➕ Computing WITH stats...\")\n",
    "    with_stats = (\n",
    "        universe\n",
    "        .filter(F.col(\"has_amenity\"))\n",
    "        .groupBy(\"target_id\", \"amenity\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"neighbor_id\").alias(\"n_with\"),\n",
    "            F.variance(value_col).alias(\"var_with\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # WITHOUT stats\n",
    "    # --------------------------------------------------\n",
    "    print(\"➖ Computing WITHOUT stats...\")\n",
    "    without_stats = (\n",
    "        universe\n",
    "        .filter(~F.col(\"has_amenity\"))\n",
    "        .groupBy(\"target_id\", \"amenity\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"neighbor_id\").alias(\"n_without\"),\n",
    "            F.variance(value_col).alias(\"var_without\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # variance of effect\n",
    "    # --------------------------------------------------\n",
    "    print(\"\uD83D\uDCCA Computing variance of effect...\")\n",
    "    variance_df = (\n",
    "        with_stats\n",
    "        .join(without_stats, [\"target_id\", \"amenity\"], \"left\")\n",
    "        .withColumn(\n",
    "            \"variance\",\n",
    "            (F.col(\"var_with\") / F.col(\"n_with\")) +\n",
    "            (F.col(\"var_without\") / F.col(\"n_without\"))\n",
    "        )\n",
    "        .select(\"target_id\", \"amenity\", \"variance\")\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # enrich recs\n",
    "    # --------------------------------------------------\n",
    "    print(\"\uD83E\uDDE0 Enriching recs with variance...\")\n",
    "    enriched = (\n",
    "        recs\n",
    "        .join(variance_df, [\"target_id\", \"amenity\"], \"left\")\n",
    "        .withColumn(\n",
    "            \"variance\",\n",
    "            F.when(\n",
    "                F.col(\"variance\").isNull() | (F.col(\"variance\") <= 0),\n",
    "                F.lit(VAR_FALLBACK)\n",
    "            ).otherwise(F.col(\"variance\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if mode == \"rating\":\n",
    "        print(\"\uD83E\uDDF9 [RATING FIX] Dropping recs where target rating is NULL/0 (safe)...\")\n",
    "        target_ratings_for_recs = (\n",
    "            amenities\n",
    "            .select(\n",
    "                F.col(\"property_id\").alias(\"target_id\"),\n",
    "                F.col(\"ratings\").alias(\"target_ratings\")\n",
    "            )\n",
    "            .dropDuplicates([\"target_id\"])\n",
    "        )\n",
    "\n",
    "        enriched = (\n",
    "            enriched\n",
    "            .join(target_ratings_for_recs, on=\"target_id\", how=\"left\")\n",
    "            .filter(F.col(\"target_ratings\").isNotNull() & (F.col(\"target_ratings\") > 0))\n",
    "            .drop(\"target_ratings\")\n",
    "        )\n",
    "\n",
    "    print(f\"✅ DONE build_effects_with_variance | mode={mode}\")\n",
    "    return enriched\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# PRICE\n",
    "# ======================================================\n",
    "\n",
    "print(\"\\n\uD83D\uDCB0 START PRICE PIPELINE\")\n",
    "price_enriched = build_effects_with_variance(\n",
    "    neighbors_path=NEIGHBORS_PRICE_PATH,\n",
    "    recs_path=RECS_PRICE_PATH,\n",
    "    amenities_path=LISTING_AMENITIES_FLAT_PATH,\n",
    "    mode=\"price\"\n",
    ")\n",
    "\n",
    "price_out = (\n",
    "    price_enriched\n",
    "    .select(\n",
    "        F.col(\"target_id\").alias(\"property_id\"),\n",
    "        \"amenity\",\n",
    "        F.col(\"score\").alias(\"delta_price\"),\n",
    "        \"variance\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\uD83D\uDCBE Writing price output...\")\n",
    "price_out.write.mode(\"overwrite\").parquet(OUT_PRICE_PATH)\n",
    "print(\"✅ Price output written\")\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# RATING\n",
    "# ======================================================\n",
    "\n",
    "print(\"\\n⭐ START RATING PIPELINE\")\n",
    "rating_enriched = build_effects_with_variance(\n",
    "    neighbors_path=NEIGHBORS_RATING_PATH,\n",
    "    recs_path=RECS_RATING_PATH,\n",
    "    amenities_path=LISTING_AMENITIES_FLAT_PATH,\n",
    "    mode=\"rating\"\n",
    ")\n",
    "\n",
    "rating_out = (\n",
    "    rating_enriched\n",
    "    .select(\n",
    "        F.col(\"target_id\").alias(\"property_id\"),\n",
    "        \"amenity\",\n",
    "        F.col(\"score\").alias(\"delta_rating\"),\n",
    "        \"variance\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\uD83D\uDCBE Writing rating output...\")\n",
    "rating_out.write.mode(\"overwrite\").parquet(OUT_RATING_PATH)\n",
    "print(\"✅ Rating output written\")\n",
    "\n",
    "print(\"\uD83D\uDED1 Spark session stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1030448-2e0e-4f5e-9b17-6900256bb422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) Helpers: normalization + safe casting\n",
    "# ============================================================\n",
    "\n",
    "def normalize_amenity_expr(col):\n",
    "    \"\"\"\n",
    "    Normalize raw amenity text (format 2) into the key style of format 1:\n",
    "      \"private patio or balcony\" -> \"a_private_patio_or_balcony\"\n",
    "    Notes:\n",
    "    - Intended for SINGLE amenities (no __x__ in format 2)\n",
    "    - Robust: lower, trim, replace punctuation with spaces, collapse spaces -> '_'\n",
    "    \"\"\"\n",
    "    c = F.lower(F.trim(col))\n",
    "\n",
    "    # Replace common punctuation / separators with space\n",
    "    c = F.regexp_replace(c, r\"[/|,;:\\(\\)\\[\\]\\{\\}\\.\\!\\?\\+\\\"'`]\", \" \")\n",
    "    c = F.regexp_replace(c, r\"[-–—]\", \" \")                 # dashes -> space\n",
    "    c = F.regexp_replace(c, r\"[&]\", \" and \")               # '&' -> 'and'\n",
    "    c = F.regexp_replace(c, r\"\\s+\", \" \")                   # collapse whitespace\n",
    "\n",
    "    # spaces -> underscores\n",
    "    c = F.regexp_replace(c, r\"\\s\", \"_\")\n",
    "\n",
    "    # remove duplicated underscores\n",
    "    c = F.regexp_replace(c, r\"_+\", \"_\")\n",
    "\n",
    "    # strip underscores at ends\n",
    "    c = F.regexp_replace(c, r\"^_+|_+$\", \"\")\n",
    "\n",
    "    # prefix with 'a_'\n",
    "    return F.concat(F.lit(\"a_\"), c)\n",
    "\n",
    "\n",
    "def is_valid_variance(col):\n",
    "    \"\"\"variance must be non-null and > 0\"\"\"\n",
    "    return col.isNotNull() & (col > F.lit(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "159f2cc1-1783-4e12-bbd1-5da3d63faf12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded inputs:\ndf_format1 columns: ['host_id', 'property_id', 'price_upgrades_all', 'rating_upgrades_all']\ndf_knn_price columns: ['property_id', 'amenity', 'delta_price', 'variance']\ndf_knn_rating columns: ['property_id', 'amenity', 'delta_rating', 'variance']\n✅ Done. Wrote:\n - dbfs:/FileStore/airbnb/bayes_price_model\n - dbfs:/FileStore/airbnb/bayes_rating_model\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# Base input paths (CHANGE THESE)\n",
    "OUTPUT_BASE = \"dbfs:/FileStore/airbnb\"\n",
    "FORMAT1_PATH = \"dbfs:/airbnb/property_upgrade_recommendations\"  \n",
    "KNN_PRICE_PATH = f\"{OUTPUT_BASE}/amenity_price_effects.parquet\"\n",
    "KNN_RATING_PATH = f\"{OUTPUT_BASE}/amenity_rating_effects.parquet\"    \n",
    "\n",
    "# Read\n",
    "df_format1 = spark.read.parquet(FORMAT1_PATH)\n",
    "df_knn_price = spark.read.parquet(KNN_PRICE_PATH)\n",
    "df_knn_rating = spark.read.parquet(KNN_RATING_PATH)\n",
    "\n",
    "print(\"✅ Loaded inputs:\")\n",
    "print(\"df_format1 columns:\", df_format1.columns)\n",
    "print(\"df_knn_price columns:\", df_knn_price.columns)\n",
    "print(\"df_knn_rating columns:\", df_knn_rating.columns)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Parse FORMAT 1 (PRIOR): explode into long tables\n",
    "# ============================================================\n",
    "\n",
    "def build_prior_long(df_format1,\n",
    "                     property_col=\"property_id\",\n",
    "                     host_col=\"host_id\",\n",
    "                     price_col=\"price_upgrades_all\",\n",
    "                     rating_col=\"rating_upgrades_all\"):\n",
    "    \"\"\"\n",
    "    FORMAT1 is already: array<struct(feature, price_delta_usd, rating_delta, coef_var, ... )>\n",
    "    We only explode.\n",
    "    Adds host_id to carry it into final outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    # PRIOR PRICE LONG\n",
    "    prior_price_long = (\n",
    "        df_format1\n",
    "        .select(\n",
    "            F.col(host_col).alias(\"host_id\"),\n",
    "            F.col(property_col).alias(\"property_id\"),\n",
    "            F.explode_outer(F.col(price_col)).alias(\"it\")\n",
    "        )\n",
    "        .select(\n",
    "            \"host_id\",\n",
    "            \"property_id\",\n",
    "            F.col(\"it.feature\").alias(\"amenity_key\"),\n",
    "            F.col(\"it.price_delta_usd\").cast(\"double\").alias(\"mu_prior\"),\n",
    "            F.col(\"it.coef_var\").cast(\"double\").alias(\"var_prior\"),   # ✅ variance = coef_var\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # PRIOR RATING LONG\n",
    "    prior_rating_long = (\n",
    "        df_format1\n",
    "        .select(\n",
    "            F.col(host_col).alias(\"host_id\"),\n",
    "            F.col(property_col).alias(\"property_id\"),\n",
    "            F.explode_outer(F.col(rating_col)).alias(\"it\")\n",
    "        )\n",
    "        .select(\n",
    "            \"host_id\",\n",
    "            \"property_id\",\n",
    "            F.col(\"it.feature\").alias(\"amenity_key\"),\n",
    "            F.col(\"it.rating_delta\").cast(\"double\").alias(\"mu_prior\"),\n",
    "            F.col(\"it.coef_var\").cast(\"double\").alias(\"var_prior\"),   # ✅ variance = coef_var\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return prior_price_long, prior_rating_long\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Parse FORMAT 2 (KNN measurement): already long, normalize amenity -> amenity_key\n",
    "# ============================================================\n",
    "\n",
    "def build_knn_long_price(df_knn_price,\n",
    "                         property_col=\"property_id\",\n",
    "                         amenity_col=\"amenity\",\n",
    "                         delta_col=\"delta_price\",\n",
    "                         variance_col=\"variance\"):\n",
    "    \"\"\"\n",
    "    Output columns:\n",
    "      property_id, amenity_key, mu_knn, var_knn, amenity_raw\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df_knn_price\n",
    "        .select(\n",
    "            F.col(property_col).alias(\"property_id\"),\n",
    "            F.col(amenity_col).alias(\"amenity_raw\"),\n",
    "            F.col(delta_col).cast(\"double\").alias(\"mu_knn\"),\n",
    "            F.col(variance_col).cast(\"double\").alias(\"var_knn\"),\n",
    "        )\n",
    "        .withColumn(\"amenity_key\", normalize_amenity_expr(F.col(\"amenity_raw\")))\n",
    "    )\n",
    "\n",
    "\n",
    "def build_knn_long_rating(df_knn_rating,\n",
    "                          property_col=\"property_id\",\n",
    "                          amenity_col=\"amenity\",\n",
    "                          delta_col=\"delta_rating\",\n",
    "                          variance_col=\"variance\"):\n",
    "    return (\n",
    "        df_knn_rating\n",
    "        .select(\n",
    "            F.col(property_col).alias(\"property_id\"),\n",
    "            F.col(amenity_col).alias(\"amenity_raw\"),\n",
    "            F.col(delta_col).cast(\"double\").alias(\"mu_knn\"),\n",
    "            F.col(variance_col).cast(\"double\").alias(\"var_knn\"),\n",
    "        )\n",
    "        .withColumn(\"amenity_key\", normalize_amenity_expr(F.col(\"amenity_raw\")))\n",
    "    )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Static Kalman / Bayesian fusion + LCB ranking score\n",
    "# ============================================================\n",
    "\n",
    "def fuse_prior_and_knn(prior_long, knn_long, model_name, k_lcb=1.0):\n",
    "    \"\"\"\n",
    "    Performs static Kalman-style Bayesian fusion:\n",
    "\n",
    "    prior:  N(mu_prior, var_prior)\n",
    "    knn:    N(mu_knn,  var_knn)\n",
    "\n",
    "    posterior:\n",
    "      K = var_prior / (var_prior + var_knn)\n",
    "      mu_post  = mu_prior + K * (mu_knn - mu_prior)\n",
    "      var_post = (1 - K) * var_prior\n",
    "\n",
    "    Rules:\n",
    "    - combos (__x__) exist only in prior -> prior-only\n",
    "    - if one side missing -> take the other\n",
    "    - if variance is null or <=0 -> treat as missing\n",
    "    \"\"\"\n",
    "\n",
    "    # mark combinations\n",
    "    prior_prepared = prior_long.withColumn(\n",
    "        \"is_combo\",\n",
    "        F.instr(F.col(\"amenity_key\"), \"__x__\") > 0\n",
    "    )\n",
    "\n",
    "    # join\n",
    "    joined = (\n",
    "        prior_prepared.alias(\"p\")\n",
    "        .join(\n",
    "            knn_long.alias(\"k\"),\n",
    "            on=[\"property_id\", \"amenity_key\"],\n",
    "            how=\"full_outer\"\n",
    "        )\n",
    "        .select(\n",
    "            # ✅ keep host_id from prior (knn doesn't have it)\n",
    "            F.col(\"p.host_id\").alias(\"host_id\"),\n",
    "\n",
    "            F.coalesce(F.col(\"p.property_id\"), F.col(\"k.property_id\")).alias(\"property_id\"),\n",
    "            F.coalesce(F.col(\"p.amenity_key\"), F.col(\"k.amenity_key\")).alias(\"amenity_key\"),\n",
    "\n",
    "            F.col(\"p.mu_prior\").alias(\"mu_prior\"),\n",
    "            F.col(\"p.var_prior\").alias(\"var_prior\"),\n",
    "\n",
    "            F.col(\"k.mu_knn\").alias(\"mu_knn\"),\n",
    "            F.col(\"k.var_knn\").alias(\"var_knn\"),\n",
    "            F.col(\"k.amenity_raw\").alias(\"amenity_raw\"),\n",
    "\n",
    "            F.coalesce(F.col(\"p.is_combo\"), F.lit(False)).alias(\"is_combo\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # validity flags\n",
    "    prior_ok = (\n",
    "        F.col(\"mu_prior\").isNotNull() &\n",
    "        F.col(\"var_prior\").isNotNull() &\n",
    "        (F.col(\"var_prior\") > 0)\n",
    "    )\n",
    "\n",
    "    knn_ok = (\n",
    "        F.col(\"mu_knn\").isNotNull() &\n",
    "        F.col(\"var_knn\").isNotNull() &\n",
    "        (F.col(\"var_knn\") > 0)\n",
    "    )\n",
    "\n",
    "    # fuse only when both exist AND not combo\n",
    "    can_fuse = prior_ok & knn_ok & (~F.col(\"is_combo\"))\n",
    "\n",
    "    # Kalman gain (static)\n",
    "    K = F.col(\"var_prior\") / (F.col(\"var_prior\") + F.col(\"var_knn\"))\n",
    "\n",
    "    mu_post = F.when(\n",
    "        can_fuse,\n",
    "        F.col(\"mu_prior\") + K * (F.col(\"mu_knn\") - F.col(\"mu_prior\"))\n",
    "    ).when(\n",
    "        prior_ok & (~knn_ok),\n",
    "        F.col(\"mu_prior\")\n",
    "    ).when(\n",
    "        knn_ok & (~prior_ok),\n",
    "        F.col(\"mu_knn\")\n",
    "    ).when(\n",
    "        prior_ok & knn_ok & F.col(\"is_combo\"),\n",
    "        F.col(\"mu_prior\")   # combos prior-only\n",
    "    ).otherwise(F.lit(None).cast(\"double\"))\n",
    "\n",
    "    var_post = F.when(\n",
    "        can_fuse,\n",
    "        (F.lit(1.0) - K) * F.col(\"var_prior\")\n",
    "    ).when(\n",
    "        prior_ok & (~knn_ok),\n",
    "        F.col(\"var_prior\")\n",
    "    ).when(\n",
    "        knn_ok & (~prior_ok),\n",
    "        F.col(\"var_knn\")\n",
    "    ).when(\n",
    "        prior_ok & knn_ok & F.col(\"is_combo\"),\n",
    "        F.col(\"var_prior\")\n",
    "    ).otherwise(F.lit(None).cast(\"double\"))\n",
    "\n",
    "    source_flag = F.when(\n",
    "        can_fuse, F.lit(\"prior+knn\")\n",
    "    ).when(\n",
    "        prior_ok & (~knn_ok), F.lit(\"prior_only\")\n",
    "    ).when(\n",
    "        knn_ok & (~prior_ok), F.lit(\"knn_only\")\n",
    "    ).when(\n",
    "        prior_ok & knn_ok & F.col(\"is_combo\"), F.lit(\"prior_only_combo\")\n",
    "    ).otherwise(F.lit(\"no_valid_data\"))\n",
    "\n",
    "    # ✅ LCB score in log space (simple + correct)\n",
    "    # Score = log(mu_post) - k * sqrt(var_post)\n",
    "    score_lcb_log = F.when(\n",
    "        (mu_post.isNotNull()) & (var_post.isNotNull()) &\n",
    "        (mu_post > 0) & (var_post > 0),\n",
    "        F.log(mu_post) - F.lit(float(k_lcb)) * F.sqrt(var_post)\n",
    "    ).otherwise(F.lit(None).cast(\"double\"))\n",
    "\n",
    "    out = (\n",
    "        joined\n",
    "        .withColumn(\"mu_post\", mu_post)\n",
    "        .withColumn(\"var_post\", var_post)\n",
    "        .withColumn(\"score_lcb_log\", score_lcb_log)\n",
    "        .withColumn(\"source_flag\", source_flag)\n",
    "        .withColumn(\"fused_flag\", can_fuse)\n",
    "        .withColumn(\"model\", F.lit(model_name))\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) END-TO-END: build two parquet models (price + rating)\n",
    "# ============================================================\n",
    "\n",
    "# prior longs (✅ now includes host_id)\n",
    "prior_price_long, prior_rating_long = build_prior_long(\n",
    "    df_format1,\n",
    "    property_col=\"property_id\",\n",
    "    host_col=\"host_id\",\n",
    "    price_col=\"price_upgrades_all\",\n",
    "    rating_col=\"rating_upgrades_all\",\n",
    ")\n",
    "\n",
    "# knn longs\n",
    "knn_price_long = build_knn_long_price(\n",
    "    df_knn_price,\n",
    "    property_col=\"property_id\",\n",
    "    amenity_col=\"amenity\",\n",
    "    delta_col=\"delta_price\",\n",
    "    variance_col=\"variance\",\n",
    ")\n",
    "\n",
    "knn_rating_long = build_knn_long_rating(\n",
    "    df_knn_rating,\n",
    "    property_col=\"property_id\",\n",
    "    amenity_col=\"amenity\",\n",
    "    delta_col=\"delta_rating\",\n",
    "    variance_col=\"variance\",\n",
    ")\n",
    "\n",
    "# fuse -> posterior models\n",
    "bayes_price_model = fuse_prior_and_knn(prior_price_long, knn_price_long, model_name=\"price\", k_lcb=1.0)\n",
    "bayes_rating_model = fuse_prior_and_knn(prior_rating_long, knn_rating_long, model_name=\"rating\", k_lcb=1.0)\n",
    "\n",
    "# Write outputs\n",
    "bayes_price_model.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/bayes_price_model\")\n",
    "bayes_rating_model.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/bayes_rating_model\")\n",
    "\n",
    "print(\"✅ Done. Wrote:\")\n",
    "print(f\" - {OUTPUT_BASE}/bayes_price_model\")\n",
    "print(f\" - {OUTPUT_BASE}/bayes_rating_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e1973f-4bdb-4938-af77-d9d53f8fd167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded outputs:\nbayes_price_model columns: ['host_id', 'property_id', 'amenity_key', 'mu_prior', 'var_prior', 'mu_knn', 'var_knn', 'amenity_raw', 'is_combo', 'mu_post', 'var_post', 'score_lcb_log', 'source_flag', 'fused_flag', 'model']\nbayes_rating_model columns: ['host_id', 'property_id', 'amenity_key', 'mu_prior', 'var_prior', 'mu_knn', 'var_knn', 'amenity_raw', 'is_combo', 'mu_post', 'var_post', 'score_lcb_log', 'source_flag', 'fused_flag', 'model']\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_BASE = \"dbfs:/FileStore/airbnb\" \n",
    "\n",
    "BAYES_PRICE_PATH  = f\"{OUTPUT_BASE}/bayes_price_model\"\n",
    "BAYES_RATING_PATH = f\"{OUTPUT_BASE}/bayes_rating_model\"\n",
    "\n",
    "bayes_price_model  = spark.read.parquet(BAYES_PRICE_PATH)\n",
    "bayes_rating_model = spark.read.parquet(BAYES_RATING_PATH)\n",
    "\n",
    "print(\"✅ Loaded outputs:\")\n",
    "print(\"bayes_price_model columns:\", bayes_price_model.columns)\n",
    "print(\"bayes_rating_model columns:\", bayes_rating_model.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84511278-3674-49e0-9eeb-5ea9501df74e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>amenity_name</th><th>n_properties</th><th>amenity_norm</th><th>matched_amenity_norm</th><th>similarity_score</th><th>estimated_cost</th><th>min_price</th><th>max_price</th><th>n_products</th><th>match_confidence</th></tr></thead><tbody><tr><td>kitchen</td><td>1817149</td><td>kitchen</td><td>kitchen</td><td>1.0000001192092896</td><td>455.0</td><td>305.0</td><td>705.0</td><td>7.0</td><td>HIGH</td></tr><tr><td>smoke alarm</td><td>1718954</td><td>smoke alarm</td><td>alarm clock</td><td>0.614111065864563</td><td>25.0</td><td>25.0</td><td>25.0</td><td>1.0</td><td>MANUAL_SAFETY</td></tr><tr><td>wifi</td><td>1715041</td><td>wifi</td><td>sonos wifi bookshelf speaker</td><td>0.4769666790962219</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>MANUAL_CONNECTIVITY</td></tr><tr><td>hot water</td><td>1650531</td><td>hot water</td><td>sink</td><td>0.43595096468925476</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>MANUAL_UTILITIES</td></tr><tr><td>essentials</td><td>1643562</td><td>essentials</td><td>starter kit</td><td>0.36616694927215576</td><td>null</td><td>null</td><td>null</td><td>null</td><td>NO_MATCH</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "kitchen",
         1817149,
         "kitchen",
         "kitchen",
         1.0000001192092896,
         455.0,
         305.0,
         705.0,
         7.0,
         "HIGH"
        ],
        [
         "smoke alarm",
         1718954,
         "smoke alarm",
         "alarm clock",
         0.614111065864563,
         25.0,
         25.0,
         25.0,
         1.0,
         "MANUAL_SAFETY"
        ],
        [
         "wifi",
         1715041,
         "wifi",
         "sonos wifi bookshelf speaker",
         0.4769666790962219,
         0.0,
         0.0,
         0.0,
         1.0,
         "MANUAL_CONNECTIVITY"
        ],
        [
         "hot water",
         1650531,
         "hot water",
         "sink",
         0.43595096468925476,
         0.0,
         0.0,
         0.0,
         1.0,
         "MANUAL_UTILITIES"
        ],
        [
         "essentials",
         1643562,
         "essentials",
         "starter kit",
         0.36616694927215576,
         null,
         null,
         null,
         null,
         "NO_MATCH"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "amenity_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "n_properties",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "amenity_norm",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "matched_amenity_norm",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "similarity_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "estimated_cost",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "min_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "max_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "n_products",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "match_confidence",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_csv = spark.read.csv(\n",
    "    \"dbfs:/data/amenity_inventory_with_ikea_price_embeddings\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "display(df_csv.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9f26feb-0cc8-406b-888a-31dd2e4624d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ============================================================\n",
    "# Attach estimated_cost from prices CSV\n",
    "# - Normalize prices CSV amenity -> amenity_key (a_...)\n",
    "# - Join on amenity_key to both bayes models\n",
    "# ============================================================\n",
    "\n",
    "COSTS_CSV_PATH = \"dbfs:/data/amenity_inventory_with_ikea_price_embeddings\"\n",
    "\n",
    "df_csv = spark.read.csv(COSTS_CSV_PATH, header=True, inferSchema=True)\n",
    "print(\"✅ Costs CSV columns:\", df_csv.columns)\n",
    "\n",
    "CSV_AMENITY_COL = \"amenity_name\"  \n",
    "\n",
    "df_costs_norm = (\n",
    "    df_csv\n",
    "    .select(\n",
    "        F.col(CSV_AMENITY_COL).alias(\"amenity_raw\"),\n",
    "        F.col(\"estimated_cost\").cast(\"double\").alias(\"estimated_cost\")\n",
    "    )\n",
    "    .filter(F.col(\"amenity_raw\").isNotNull())\n",
    "    .withColumn(\"amenity_key\", normalize_amenity_expr(F.col(\"amenity_raw\"))) \n",
    "    .groupBy(\"amenity_key\")\n",
    "    .agg(F.first(\"estimated_cost\", ignorenulls=True).alias(\"estimated_cost\"))\n",
    ")\n",
    "\n",
    "if \"estimated_cost\" in bayes_price_model.columns:\n",
    "    bayes_price_model = bayes_price_model.drop(\"estimated_cost\")\n",
    "if \"estimated_cost\" in bayes_rating_model.columns:\n",
    "    bayes_rating_model = bayes_rating_model.drop(\"estimated_cost\")\n",
    "\n",
    "bayes_price_model = bayes_price_model.join(df_costs_norm, on=\"amenity_key\", how=\"left\")\n",
    "bayes_rating_model = bayes_rating_model.join(df_costs_norm, on=\"amenity_key\", how=\"left\")\n",
    "\n",
    "print(\"✅ Added estimated_cost to both bayes models\")\n",
    "\n",
    "# Write final outputs\n",
    "bayes_price_model.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/bayes_price_model_final\")\n",
    "bayes_rating_model.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/bayes_rating_model_final\")\n",
    "print(\"✅ Wrote final models including estimated_cost\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6689601685948111,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "KNN ּ+ Local Analysis + Kalman + Amenities Cost",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}