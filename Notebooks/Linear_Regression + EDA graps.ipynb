{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8178a26c-a5e8-4803-b1e6-be10383f5742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Loading datasets into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c71d2f-7062-40f0-850e-85c42f5b1197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account = \"replace_by_storage_account\"  \n",
    "container = \"replace_by_container\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5877cd9-f2d5-4cd4-b72f-4586a0e6aa86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading airbnb data from Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8f8b87-a2ce-479e-af65-8d702b522fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sas_token=\"replace_with_your_sas_token\"\n",
    "sas_token = sas_token.lstrip('?')\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(f\"fs.azure.sas.token.provider.type.{storage_account}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.sas.fixed.token.{storage_account}.dfs.core.windows.net\", sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290cef33-116a-4dd1-8f7d-9e1b541017b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/airbnb_1_12_parquet\"\n",
    "\n",
    "airbnb = spark.read.parquet(path)\n",
    "display(airbnb.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60f44c08-3bef-493c-b10a-929c3fd33ec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ad4f33-d643-4024-9048-f7c11f523419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setup & Config\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "\n",
    "# =============================\n",
    "# GLOBAL CONFIGURATION / HYPERPARAMETERS\n",
    "# =============================\n",
    "\n",
    "# -------- Data filtering --------\n",
    "MIN_REVIEWS = 10\n",
    "# Minimum number of reviews required for a listing to be included in the analysis.\n",
    "# Higher values increase reliability but reduce sample size.\n",
    "\n",
    "# -------- Ridge regression --------\n",
    "RIDGE_ALPHAS = [0.1, 1.0, 10.0, 50.0]\n",
    "# Candidate regularization strengths for RidgeCV.\n",
    "# Higher values = stronger regularization (more bias, less variance).\n",
    "\n",
    "# -------- Bootstrap confidence --------\n",
    "N_BOOTSTRAP = 200\n",
    "# Number of bootstrap resamples used to estimate confidence.\n",
    "# Higher = more stable confidence estimates, slower runtime.\n",
    "\n",
    "BOOTSTRAP_RANDOM_STATE = 42\n",
    "# Random seed for reproducible bootstrap confidence scores.\n",
    "\n",
    "# -------- Recommendation limits --------\n",
    "TOP_K_SINGLE = 3\n",
    "# Number of single-amenity upgrade recommendations per category.\n",
    "\n",
    "TOP_K_INTERACTION = 3\n",
    "# Number of interaction-based upgrade recommendations.\n",
    "\n",
    "# -------- Scoring weights --------\n",
    "WEIGHT_PRICE = 0.7\n",
    "WEIGHT_RATING = 0.3\n",
    "# Relative importance of price vs rating in OVERALL recommendations.\n",
    "# Must sum to 1.0.\n",
    "\n",
    "# -------- Rating constraints --------\n",
    "MAX_RATING = 5.0\n",
    "# Upper bound for ratings (used to avoid impossible values like >5.0).\n",
    "\n",
    "# -------- Expected rating model config --------\n",
    "USE_PRICE_IN_EXPECTED_RATING = True\n",
    "# If True, expected-rating model will explicitly depend on price (log_price),\n",
    "# reflecting that guests form higher expectations for more expensive listings.\n",
    "\n",
    "PRICE_COL = \"price_per_night\"\n",
    "\n",
    "# =============================\n",
    "# Spark session\n",
    "# =============================\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"AmenityUpgradeAdvisor-LinearRegression\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# 0) Load CSV\n",
    "# =============================\n",
    "df = airbnb.drop(\"price\") if \"price\" in airbnb.columns else airbnb # drop \"price\" if it exists\n",
    "\n",
    "# Prefer property_id as the primary identifier\n",
    "ID_COL_CANDIDATES = [\"property_id\", \"listing_id\", \"id\"]\n",
    "id_col = next((c for c in ID_COL_CANDIDATES if c in df.columns), None)\n",
    "\n",
    "if id_col is None:\n",
    "    df = df.withColumn(\"property_id\", F.monotonically_increasing_id())\n",
    "    id_col = \"property_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619a8498-cb62-4e00-9fcc-548e603bf44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parse JSON + amenity_name_value\n",
    "\n",
    "# =============================\n",
    "# 1) Parse JSON columns\n",
    "# =============================\n",
    "amenities_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"group_name\", StringType(), True),\n",
    "        StructField(\"items\", ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"name\", StringType(), True),\n",
    "                StructField(\"value\", StringType(), True)\n",
    "            ])\n",
    "        ), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "seller_schema = StructType([\n",
    "    StructField(\"seller_id\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "])\n",
    "\n",
    "pricing_schema = StructType([\n",
    "    StructField(\"airbnb_service_fee\", DoubleType(), True),\n",
    "    StructField(\"aairbnb_service_fee\", DoubleType(), True),  \n",
    "    StructField(\"cleaning_fee\", DoubleType(), True),\n",
    "    StructField(\"initial_price_per_night\", DoubleType(), True),\n",
    "    StructField(\"num_of_nights\", IntegerType(), True),\n",
    "    StructField(\"price_per_night\", DoubleType(), True),\n",
    "    StructField(\"price_without_fees\", DoubleType(), True),\n",
    "    StructField(\"special_offer\", DoubleType(), True),\n",
    "    StructField(\"taxes\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    .withColumn(\"ratings\", F.expr(\"try_cast(ratings as double)\"))\n",
    "\n",
    "    .withColumn(\"pricing_parsed\", F.from_json(F.col(\"pricing_details\"), pricing_schema))\n",
    "    .withColumn(\"price_per_night\", F.col(\"pricing_parsed.price_per_night\"))\n",
    "\n",
    "    .withColumn(\"amenities_parsed\", F.from_json(F.col(\"amenities\"), amenities_schema))\n",
    "    .withColumn(\"reviews_parsed\", F.from_json(F.col(\"reviews\"), ArrayType(StringType())))\n",
    "    .withColumn(\"num_reviews\", F.size(F.col(\"reviews_parsed\")))\n",
    "    .withColumn(\"seller_parsed\", F.from_json(F.col(\"seller_info\"), seller_schema))\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"host_id\", F.col(\"seller_parsed.seller_id\"))\n",
    "\n",
    "# Flatten amenities to (name, value) pairs (lowercase)\n",
    "df = df.withColumn(\n",
    "    \"amenity_name_value\",\n",
    "    F.expr(\"\"\"\n",
    "        transform(\n",
    "            flatten(\n",
    "                transform(amenities_parsed, g -> g.items)\n",
    "            ),\n",
    "            x -> struct(\n",
    "                lower(x.name) as name,\n",
    "                lower(x.value) as value\n",
    "            )\n",
    "        )\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(\"dbfs:/airbnb/df_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11eb7967-5e51-466d-9af8-47f37ff67f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Parsed Base DF\n",
    "df = spark.read.parquet(\"dbfs:/airbnb/df_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf71b739-7ab6-44e4-a245-cb49643cb710",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Amenity Inventory\n",
    "\n",
    "amenities_exploded = (\n",
    "    df\n",
    "    .select(\n",
    "        F.col(\"property_id\"),\n",
    "        F.explode(\"amenity_name_value\").alias(\"a\")\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"property_id\"),\n",
    "        F.col(\"a.name\").alias(\"amenity_name\")\n",
    "    )\n",
    ")\n",
    "\n",
    "amenity_inventory = (\n",
    "    amenities_exploded\n",
    "    .groupBy(\"amenity_name\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"property_id\").alias(\"n_properties\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"n_properties\"))\n",
    ")\n",
    "\n",
    "display(amenity_inventory)\n",
    "amenity_inventory.write.mode(\"overwrite\").parquet(\"dbfs:/airbnb/amenity_inventory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "500e46ba-f798-47cc-bc18-cee89713efbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Amenity Inventory + select amenities\n",
    "amenity_inventory = spark.read.parquet(\n",
    "    \"dbfs:/airbnb/amenity_inventory\"\n",
    ")\n",
    "\n",
    "MIN_PROPERTIES = 2000\n",
    "\n",
    "selected_amenities_df = (\n",
    "    amenity_inventory\n",
    "    .filter(F.col(\"n_properties\") >= MIN_PROPERTIES)\n",
    ")\n",
    "\n",
    "amenities_to_test = [\n",
    "    r[\"amenity_name\"] for r in selected_amenities_df.collect()\n",
    "]\n",
    "\n",
    "print(f\"{len(amenities_to_test)} amenities selected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4c731a-a11e-4924-bf5a-13fea478f261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add Amenity Flags\n",
    "\n",
    "def col_name(a: str) -> str:\n",
    "    return \"a_\" + \"\".join(ch if ch.isalnum() else \"_\" for ch in a.lower())\n",
    "\n",
    "def add_amenity_flag(df_in, amenity: str):\n",
    "    a = amenity.lower()\n",
    "    c = col_name(a)\n",
    "\n",
    "    return df_in.withColumn(\n",
    "        c,\n",
    "        F.when(\n",
    "            F.exists(\n",
    "                F.col(\"amenity_name_value\"),\n",
    "                lambda x: (\n",
    "                    F.contains(x[\"name\"], F.lit(a)) &\n",
    "                    F.regexp_like(x[\"value\"], F.lit(\"no_\"))\n",
    "                )\n",
    "            ),\n",
    "            F.lit(0)\n",
    "        ).when(\n",
    "            F.exists(\n",
    "                F.col(\"amenity_name_value\"),\n",
    "                lambda x: (\n",
    "                    F.contains(x[\"name\"], F.lit(a)) &\n",
    "                    (~F.regexp_like(x[\"value\"], F.lit(\"no_\")))\n",
    "                )\n",
    "            ),\n",
    "            F.lit(1)\n",
    "        ).otherwise(F.lit(0))\n",
    "    )\n",
    "\n",
    "for a in amenities_to_test:\n",
    "    df = add_amenity_flag(df, a)\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(\"dbfs:/airbnb/df_with_amenities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e534db-14b7-45f5-98dc-c58aa90bd232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load DF with Amenities\n",
    "df = spark.read.parquet(\"dbfs:/airbnb/df_with_amenities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4e04aa-c5c5-499b-83fa-95866f62d792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter + Feature Engineering\n",
    "# Collect actual amenity columns that exist in the dataframe\n",
    "amenity_cols_existing = [c for c in df.columns if c.startswith(\"a_\")]\n",
    "amenity_set = set(amenity_cols_existing)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 3) Filter for reliable analysis + print breakdown (ONCE)\n",
    "# =============================\n",
    "\n",
    "total_raw = df.count()\n",
    "\n",
    "df_price_ok = df.filter(F.col(PRICE_COL).isNotNull() & (F.col(PRICE_COL) > 0))\n",
    "n_price_ok = df_price_ok.count()\n",
    "\n",
    "df_rating_ok = df_price_ok.filter(F.col(\"ratings\").isNotNull())\n",
    "n_rating_ok = df_rating_ok.count()\n",
    "\n",
    "df_reviews_ok = df_rating_ok.filter(F.col(\"num_reviews\") >= MIN_REVIEWS)\n",
    "n_reviews_ok = df_reviews_ok.count()\n",
    "\n",
    "df_f = df_reviews_ok.withColumn(\"log_price\", F.log(F.col(PRICE_COL)))\n",
    "\n",
    "# Cache the filtered, feature-rich dataframe (used many times later)\n",
    "df_f = df_f.cache()\n",
    "df_f.count()  # materialize cache\n",
    "\n",
    "print(\"\\n=== Analysis Sample Explanation ===\")\n",
    "print(f\"Total listings in raw dataset: {total_raw}\")\n",
    "print(f\"Listings used for statistical analysis: {n_reviews_ok}\")\n",
    "print(\"\\nListings were INCLUDED in the analysis only if they satisfy ALL of the following:\")\n",
    "print(f\"  • After price_per_night filter (price_per_night > 0): {n_price_ok} (removed {total_raw - n_price_ok})\")\n",
    "print(f\"  • After rating filter (rating available): {n_rating_ok} (removed {n_price_ok - n_rating_ok})\")\n",
    "print(f\"  • After review-count filter (≥ {MIN_REVIEWS} reviews): {n_reviews_ok} (removed {n_rating_ok - n_reviews_ok})\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 3.5) Control features (non-actionable)\n",
    "# =============================\n",
    "df_f = (\n",
    "    df_f\n",
    "    .withColumn(\"guests\", F.expr(\"try_cast(guests as int)\"))\n",
    "    .withColumn(\n",
    "        \"num_beds\",\n",
    "        F.when(\n",
    "            F.col(\"arrangement_details\").isNotNull(),\n",
    "            F.regexp_count(F.lower(F.col(\"arrangement_details\")), F.lit(\"bed\"))\n",
    "        ).otherwise(0)\n",
    "    )\n",
    "    .withColumn(\"is_superhost\", F.expr(\"CASE WHEN is_supperhost = TRUE THEN 1 ELSE 0 END\"))\n",
    "    .withColumn(\"host_rating\", F.expr(\"try_cast(host_rating as double)\"))\n",
    "    .withColumn(\"host_reviews\", F.expr(\"try_cast(host_number_of_reviews as int)\"))\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# Amenity interaction features\n",
    "# =============================\n",
    "INTERACTION_PAIRS = [\n",
    "    # Work / business\n",
    "    (\"a_wifi\", \"a_dedicated_workspace\"),\n",
    "    (\"a_wifi\", \"a_self_check_in\"),\n",
    "\n",
    "    # Kitchen completeness\n",
    "    (\"a_kitchen\", \"a_cooking_basics\"),\n",
    "    (\"a_kitchen\", \"a_dishwasher\"),\n",
    "    (\"a_oven\", \"a_baking_sheet\"),\n",
    "    (\"a_coffee\", \"a_kettle\"),\n",
    "\n",
    "    # Outdoor / leisure\n",
    "    (\"a_patio_or_balcony\", \"a_outdoor_furniture\"),\n",
    "    (\"a_bbq_grill\", \"a_outdoor_dining_area\"),\n",
    "    (\"a_backyard\", \"a_bbq_grill\"),\n",
    "    (\"a_private_backyard___fully_fenced\", \"a_pets_allowed\"),\n",
    "\n",
    "    # Views\n",
    "    (\"a_sea_view\", \"a_patio_or_balcony\"),\n",
    "    (\"a_city_skyline_view\", \"a_patio_or_balcony\"),\n",
    "    (\"a_garden_view\", \"a_outdoor_furniture\"),\n",
    "\n",
    "    # Long stay\n",
    "    (\"a_washing_machine\", \"a_drying_rack_for_clothing\"),\n",
    "    (\"a_washing_machine\", \"a_iron\"),\n",
    "\n",
    "    # Family\n",
    "    (\"a_crib\", \"a_high_chair\"),\n",
    "    (\"a_children_s_books_and_toys\", \"a_children_s_dinnerware\"),\n",
    "\n",
    "    # Comfort\n",
    "    (\"a_bed_linens\", \"a_extra_pillows_and_blankets\"),\n",
    "    (\"a_room_darkening_shades\", \"a_ceiling_fan\"),\n",
    "\n",
    "    # Safety / trust\n",
    "    (\"a_smoke_alarm\", \"a_fire_extinguisher\"),\n",
    "    (\"a_first_aid_kit\", \"a_smoke_alarm\"),\n",
    "]\n",
    "\n",
    "# Create ONLY valid interaction features (both sides must exist)\n",
    "valid_interactions = []\n",
    "\n",
    "for a, b in INTERACTION_PAIRS:\n",
    "    if a in amenity_set and b in amenity_set:\n",
    "        inter_col = f\"{a}__x__{b}\"\n",
    "        df_f = df_f.withColumn(inter_col, F.col(a) * F.col(b))\n",
    "        valid_interactions.append(inter_col)\n",
    "\n",
    "print(\"Created interaction features\")\n",
    "\n",
    "# =============================\n",
    "# 4) Build Pandas dataset for regression\n",
    "# =============================\n",
    "amenity_features = [col_name(a) for a in amenities_to_test]\n",
    "\n",
    "# NON_ACTIONABLE amenities (not actionable for upgrade recommendations)\n",
    "NON_ACTIONABLE = {\n",
    "    \"a_sea_view\",\n",
    "    \"a_city_skyline_view\",\n",
    "    \"a_garden_view\",\n",
    "    \"a_elevator\"\n",
    "}\n",
    "\n",
    "# Interaction features\n",
    "interaction_features = valid_interactions\n",
    "\n",
    "control_features = [\n",
    "    \"guests\",\n",
    "    \"num_beds\",\n",
    "    \"is_superhost\",\n",
    "    \"host_rating\",\n",
    "    \"host_reviews\"\n",
    "]\n",
    "\n",
    "feature_cols = [c for c in (amenity_features + interaction_features + control_features) if c in df_f.columns]\n",
    "\n",
    "df_f = (\n",
    "    df_f\n",
    "    .withColumn(\"desc_len\", F.length(F.coalesce(F.col(\"description\"), F.lit(\"\"))))\n",
    "    .withColumn(\"num_reviews_text\", F.coalesce(F.size(F.col(\"reviews_parsed\")), F.lit(0)))\n",
    ")\n",
    "\n",
    "\n",
    "OUTPUT_PATH = \"dbfs:/airbnb/df_features_v1\"\n",
    "\n",
    "df_f.write.mode(\"overwrite\").parquet(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34e0978-8f5d-4f2e-939c-dc3e14a744f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load Features\n",
    "df_f = spark.read.parquet(\"dbfs:/airbnb/df_features_v1\")\n",
    "\n",
    "\n",
    "# Add these features to the feature list\n",
    "feature_cols += [\"desc_len\", \"num_reviews_text\"]\n",
    "\n",
    "# Define rating feature list (includes log_price)\n",
    "feature_cols_rating = feature_cols.copy()\n",
    "if \"log_price\" not in feature_cols_rating:\n",
    "    feature_cols_rating.append(\"log_price\")\n",
    "\n",
    "# --- Train on a Spark sample only (NOT full dataset) ---\n",
    "MODEL_SAMPLE_FRACTION = 0.1  # 10% (tune if you want)\n",
    "\n",
    "df_model = (\n",
    "    df_f\n",
    "    .sample(withReplacement=False, fraction=MODEL_SAMPLE_FRACTION, seed=42)\n",
    "    .select([\"ratings\"] + feature_cols_rating)\n",
    ")\n",
    "\n",
    "pdf_model = df_model.toPandas()\n",
    "pdf_model = pdf_model.loc[:, ~pdf_model.columns.duplicated()]\n",
    "\n",
    "# Training subsets\n",
    "pdf_price = pdf_model.dropna(subset=[\"log_price\"]).copy()\n",
    "pdf_rating = pdf_model.dropna(subset=[\"ratings\"]).copy()\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Helper functions for upgrade improvement calculations\n",
    "# =============================\n",
    "def price_improvement(current_price_per_night, price_multiplier):\n",
    "    new_price_per_night = current_price_per_night * price_multiplier\n",
    "    pct = (new_price_per_night / current_price_per_night - 1.0) * 100.0 if current_price_per_night > 0 else None\n",
    "    return new_price_per_night, pct\n",
    "\n",
    "def rating_improvement(current_rating, delta):\n",
    "    new_rating = min(current_rating + delta, MAX_RATING)\n",
    "    pct = ((new_rating - current_rating) / current_rating) * 100.0 if current_rating > 0 else None\n",
    "    return new_rating, pct\n",
    "\n",
    "# (property_id is now always present earlier)\n",
    "\n",
    "# ===== Define feature_cols_rating for rating model (with log_price) =====\n",
    "feature_cols_rating = feature_cols.copy()\n",
    "if \"log_price\" not in feature_cols_rating:\n",
    "    feature_cols_rating.append(\"log_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bec6db5f-7228-4069-8968-179711ed6dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Modeling + Bootstrap\n",
    "# =============================\n",
    "# 5) Ridge Regression - Price model: log(price)\n",
    "# =============================\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import resample\n",
    "\n",
    "price_feature_order = feature_cols.copy()\n",
    "\n",
    "X_price = pdf_price[price_feature_order].fillna(0.0)\n",
    "\n",
    "y_price = pdf_price[\"log_price\"]\n",
    "\n",
    "ridge = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", RidgeCV(alphas=RIDGE_ALPHAS))\n",
    "])\n",
    "\n",
    "ridge.fit(X_price, y_price)\n",
    "\n",
    "# =============================\n",
    "# Bootstrap-based confidence helper\n",
    "# =============================\n",
    "def bootstrap_variance_ridge(\n",
    "    X, y, feature_names, base_pipeline,\n",
    "    n_boot=N_BOOTSTRAP,\n",
    "    random_state=BOOTSTRAP_RANDOM_STATE\n",
    "):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    coef_samples = {f: [] for f in feature_names}\n",
    "\n",
    "    for i in range(n_boot):\n",
    "        X_b, y_b = resample(X, y, random_state=rng.randint(1e9))\n",
    "\n",
    "        model = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"ridge\", Ridge(alpha=base_pipeline.named_steps[\"ridge\"].alpha_))\n",
    "        ])\n",
    "        model.fit(X_b, y_b)\n",
    "\n",
    "        coefs = model.named_steps[\"ridge\"].coef_.ravel()\n",
    "\n",
    "        for f, c in zip(feature_names, coefs):\n",
    "            coef_samples[f].append(c)\n",
    "\n",
    "    stats = {}\n",
    "    for f, values in coef_samples.items():\n",
    "        v = np.array(values)\n",
    "        stats[f] = {\n",
    "            \"mean\": v.mean(),\n",
    "            \"var\": v.var(ddof=1),\n",
    "            \"std\": v.std(ddof=1),\n",
    "            \"ci_low\": np.percentile(v, 2.5),\n",
    "            \"ci_high\": np.percentile(v, 97.5)\n",
    "        }\n",
    "\n",
    "    return stats\n",
    "\n",
    "price_stats = bootstrap_variance_ridge(\n",
    "    X_price.values,\n",
    "    y_price.values,\n",
    "    feature_cols,\n",
    "    ridge\n",
    ")\n",
    "\n",
    "coef_price = ridge.named_steps[\"ridge\"].coef_.reshape(-1)\n",
    "\n",
    "price_table = pd.DataFrame({\n",
    "    \"feature\": X_price.columns.tolist(),\n",
    "    \"coef_log_price\": coef_price,\n",
    "})\n",
    "\n",
    "price_table[\"price_multiplier\"] = np.exp(price_table[\"coef_log_price\"])\n",
    "price_table[\"coef_std\"] = price_table[\"feature\"].map(lambda f: price_stats[f][\"std\"])\n",
    "price_table[\"coef_var\"] = price_table[\"feature\"].map(lambda f: price_stats[f][\"var\"])\n",
    "price_table[\"ci_low\"] = price_table[\"feature\"].map(lambda f: price_stats[f][\"ci_low\"])\n",
    "price_table[\"ci_high\"] = price_table[\"feature\"].map(lambda f: price_stats[f][\"ci_high\"])\n",
    "\n",
    "# =============================\n",
    "# 6) Ridge Regression - Rating model\n",
    "# =============================\n",
    "rating_feature_order = feature_cols_rating.copy()\n",
    "\n",
    "X_rating = pdf_rating[rating_feature_order].fillna(0.0)\n",
    "\n",
    "y_rating = pdf_rating[\"ratings\"]\n",
    "\n",
    "ridge_rating = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", RidgeCV(alphas=RIDGE_ALPHAS))\n",
    "])\n",
    "\n",
    "ridge_rating.fit(X_rating, y_rating)\n",
    "\n",
    "# Print verification for log_price in rating model\n",
    "print(f\"Rating model includes log_price: {'log_price' in feature_cols_rating}\")\n",
    "\n",
    "rating_stats = bootstrap_variance_ridge(\n",
    "    X_rating.values,\n",
    "    y_rating.values,\n",
    "    feature_cols_rating,\n",
    "    ridge_rating\n",
    ")\n",
    "\n",
    "coef_rating = ridge_rating.named_steps[\"ridge\"].coef_.reshape(-1)\n",
    "\n",
    "rating_table = pd.DataFrame({\n",
    "    \"feature\": X_rating.columns.tolist(),\n",
    "    \"coef_rating\": coef_rating,\n",
    "})\n",
    "\n",
    "rating_table[\"coef_std\"] = rating_table[\"feature\"].map(lambda f: rating_stats[f][\"std\"])\n",
    "rating_table[\"coef_var\"] = rating_table[\"feature\"].map(lambda f: rating_stats[f][\"var\"])\n",
    "rating_table[\"ci_low\"] = rating_table[\"feature\"].map(lambda f: rating_stats[f][\"ci_low\"])\n",
    "rating_table[\"ci_high\"] = rating_table[\"feature\"].map(lambda f: rating_stats[f][\"ci_high\"])\n",
    "\n",
    "# =============================\n",
    "# 7) Concise conclusions table (amenities only)\n",
    "# =============================\n",
    "final_conclusions = (\n",
    "    price_table\n",
    "    .merge(\n",
    "        rating_table,\n",
    "        on=\"feature\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_price\", \"_rating\")\n",
    "    )\n",
    "    .query(\"feature in @amenity_features\")\n",
    "    .rename(columns={\"feature\": \"amenity\"})\n",
    "    .fillna(0.0)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n=== Final Amenity Impact Conclusions (Ridge Regression, Controlled) ===\")\n",
    "print(\"Interpretation: multiplicative price effect & additive rating effect, controlling for size, host, and text quality.\")\n",
    "print(final_conclusions.to_string(index=False))\n",
    "\n",
    "# =============================\n",
    "# 7.1) Interaction effect tables (explicit output)\n",
    "# =============================\n",
    "interaction_price_table = (\n",
    "    price_table\n",
    "    .query(\"feature in @interaction_features\")\n",
    "    .sort_values(\"price_multiplier\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "interaction_rating_table = (\n",
    "    rating_table\n",
    "    .query(\"feature in @interaction_features\")\n",
    "    .sort_values(\"coef_rating\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n=== Amenity Interaction Effects – PRICE ===\")\n",
    "print(\"Interpretation: joint effect beyond individual amenities.\")\n",
    "if interaction_price_table.empty:\n",
    "    print(\"No interaction effects available.\")\n",
    "else:\n",
    "    print(interaction_price_table.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== Amenity Interaction Effects – RATING ===\")\n",
    "print(\"Interpretation: joint effect on guest satisfaction.\")\n",
    "if interaction_rating_table.empty:\n",
    "    print(\"No interaction effects available.\")\n",
    "else:\n",
    "    print(interaction_rating_table.to_string(index=False))\n",
    "\n",
    "\n",
    "spark_final_conclusions = spark.createDataFrame(final_conclusions)\n",
    "spark_interaction_price = spark.createDataFrame(interaction_price_table)\n",
    "spark_interaction_rating = spark.createDataFrame(interaction_rating_table)\n",
    "\n",
    "spark_final_conclusions.write.mode(\"overwrite\").parquet(\"dbfs:/airbnb/final_conclusions\")\n",
    "spark_interaction_price.write.mode(\"overwrite\").parquet(\"dbfs:/airbnb/interaction_effects_price\")\n",
    "spark_interaction_rating.write.mode(\"overwrite\").parquet(\"dbfs:/airbnb/interaction_effects_rating\")\n",
    "\n",
    "spark.createDataFrame(price_table).write.mode(\"overwrite\").parquet(\"dbfs:/airbnb/price_table\")\n",
    "spark.createDataFrame(rating_table).write.mode(\"overwrite\").parquet(\"dbfs:/airbnb/rating_table\")\n",
    "\n",
    "print(\"Analysis tables saved to DBFS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57121c74-4a78-4fef-975c-fc8c60e8781e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  Load Model Outputs\n",
    "price_table = spark.read.parquet(\"dbfs:/airbnb/price_table\").toPandas()\n",
    "rating_table = spark.read.parquet(\"dbfs:/airbnb/rating_table\").toPandas()\n",
    "\n",
    "# Property-level Recommendations\n",
    "\n",
    "# =============================\n",
    "# 9) PROPERTY-LEVEL UPGRADE RECOMMENDATIONS (SPARK-SAFE)\n",
    "# =============================\n",
    "\n",
    "# --------------------------------\n",
    "# Prepare Python-side lookup tables\n",
    "# --------------------------------\n",
    "\n",
    "# --------------------------------\n",
    "# SAFETY: ensure unique feature rows\n",
    "# --------------------------------\n",
    "price_table = (\n",
    "    price_table\n",
    "    .groupby(\"feature\", as_index=False)\n",
    "    .first()\n",
    ")\n",
    "\n",
    "rating_table = (\n",
    "    rating_table\n",
    "    .groupby(\"feature\", as_index=False)\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Optional sanity check\n",
    "assert price_table[\"feature\"].is_unique\n",
    "assert rating_table[\"feature\"].is_unique\n",
    "\n",
    "price_table_py = price_table.set_index(\"feature\").to_dict(\"index\")\n",
    "rating_table_py = rating_table.set_index(\"feature\").to_dict(\"index\")\n",
    "price_stats_bc = spark.sparkContext.broadcast(price_stats)\n",
    "rating_stats_bc = spark.sparkContext.broadcast(rating_stats)\n",
    "\n",
    "# --------------------------------\n",
    "# Broadcast everything needed\n",
    "# --------------------------------\n",
    "price_table_bc = spark.sparkContext.broadcast(price_table_py)\n",
    "rating_table_bc = spark.sparkContext.broadcast(rating_table_py)\n",
    "amenity_features_bc = spark.sparkContext.broadcast(set(amenity_features))\n",
    "interaction_features_bc = spark.sparkContext.broadcast(set(interaction_features))\n",
    "non_actionable_bc = spark.sparkContext.broadcast(set(NON_ACTIONABLE))\n",
    "\n",
    "# --------------------------------\n",
    "# Core computation per property\n",
    "# --------------------------------\n",
    "def compute_recommendations_for_row(row):\n",
    "    row = row.asDict()\n",
    "\n",
    "    pid = int(row[\"property_id\"]) if row.get(\"property_id\") is not None else None\n",
    "    host_id = row.get(\"host_id\")\n",
    "\n",
    "    current_price_per_night = row.get(PRICE_COL)\n",
    "    current_rating = row.get(\"ratings\")\n",
    "\n",
    "    if current_price_per_night is None or current_rating is None:\n",
    "        return {\n",
    "            \"host_id\": host_id,\n",
    "            \"property_id\": pid,\n",
    "            \"price_upgrades_all\": [],\n",
    "            \"rating_upgrades_all\": [],\n",
    "        }\n",
    "\n",
    "    price_upgrades = []\n",
    "    rating_upgrades = []\n",
    "    combined = []\n",
    "\n",
    "    price_dict = {}\n",
    "    rating_dict = {}\n",
    "\n",
    "    # ---------- PRICE: single amenities ----------\n",
    "    for f, r in price_table_bc.value.items():\n",
    "        if (\n",
    "            f in amenity_features_bc.value\n",
    "            and f not in non_actionable_bc.value\n",
    "            and row.get(f, 0) == 0\n",
    "            and r[\"coef_log_price\"] > 0\n",
    "        ):\n",
    "            stat = price_stats_bc.value[f]\n",
    "            price_delta = current_price_per_night * (math.exp(r[\"coef_log_price\"]) - 1)\n",
    "            price_upgrades.append({\n",
    "                \"feature\": f,\n",
    "                \"price_delta_usd\": float(price_delta),\n",
    "                \"coef_mean\": float(stat[\"mean\"]),\n",
    "                \"coef_std\": float(stat[\"std\"]),\n",
    "                \"coef_var\": float(stat[\"var\"]),\n",
    "                \"ci_low\": float(stat[\"ci_low\"]),\n",
    "                \"ci_high\": float(stat[\"ci_high\"]),\n",
    "            })\n",
    "\n",
    "    # ---------- PRICE: interactions ----------\n",
    "    for f, r in price_table_bc.value.items():\n",
    "        if f in interaction_features_bc.value and r[\"coef_log_price\"] > 0:\n",
    "            a, b = f.split(\"__x__\")\n",
    "            if (\n",
    "                row.get(a, 0) == 0 and row.get(b, 0) == 0\n",
    "                and a not in non_actionable_bc.value\n",
    "                and b not in non_actionable_bc.value\n",
    "            ):\n",
    "                stat = price_stats_bc.value[f]\n",
    "                price_delta = current_price_per_night * (math.exp(r[\"coef_log_price\"]) - 1)\n",
    "                price_upgrades.append({\n",
    "                    \"feature\": f,\n",
    "                    \"price_delta_usd\": float(price_delta),\n",
    "                    \"coef_mean\": float(stat[\"mean\"]),\n",
    "                    \"coef_std\": float(stat[\"std\"]),\n",
    "                    \"coef_var\": float(stat[\"var\"]),\n",
    "                    \"ci_low\": float(stat[\"ci_low\"]),\n",
    "                    \"ci_high\": float(stat[\"ci_high\"]),\n",
    "                })\n",
    "\n",
    "    # ---------- RATING: single amenities ----------\n",
    "    for f, r in rating_table_bc.value.items():\n",
    "        if (\n",
    "            f in amenity_features_bc.value\n",
    "            and f not in non_actionable_bc.value\n",
    "            and row.get(f, 0) == 0\n",
    "            and r[\"coef_rating\"] > 0\n",
    "        ):\n",
    "            stat = rating_stats_bc.value[f]\n",
    "            rating_delta = min(r[\"coef_rating\"], MAX_RATING - current_rating)\n",
    "            rating_upgrades.append({\n",
    "                \"feature\": f,\n",
    "                \"rating_delta\": float(rating_delta),\n",
    "                \"coef_mean\": float(stat[\"mean\"]),\n",
    "                \"coef_std\": float(stat[\"std\"]),\n",
    "                \"coef_var\": float(stat[\"var\"]),\n",
    "                \"ci_low\": float(stat[\"ci_low\"]),\n",
    "                \"ci_high\": float(stat[\"ci_high\"]),\n",
    "            })\n",
    "\n",
    "    # ---------- RATING: interactions ----------\n",
    "    for f, r in rating_table_bc.value.items():\n",
    "        if f in interaction_features_bc.value and r[\"coef_rating\"] > 0:\n",
    "            a, b = f.split(\"__x__\")\n",
    "            if (\n",
    "                row.get(a, 0) == 0 and row.get(b, 0) == 0\n",
    "                and a not in non_actionable_bc.value\n",
    "                and b not in non_actionable_bc.value\n",
    "            ):\n",
    "                stat = rating_stats_bc.value[f]\n",
    "                rating_delta = min(r[\"coef_rating\"], MAX_RATING - current_rating)\n",
    "                rating_upgrades.append({\n",
    "                    \"feature\": f,\n",
    "                    \"rating_delta\": float(rating_delta),\n",
    "                    \"coef_mean\": float(stat[\"mean\"]),\n",
    "                    \"coef_std\": float(stat[\"std\"]),\n",
    "                    \"coef_var\": float(stat[\"var\"]),\n",
    "                    \"ci_low\": float(stat[\"ci_low\"]),\n",
    "                    \"ci_high\": float(stat[\"ci_high\"]),\n",
    "                })\n",
    "\n",
    "    return {\n",
    "        \"host_id\": host_id,\n",
    "        \"property_id\": pid,\n",
    "        \"price_upgrades_all\": sorted(\n",
    "            price_upgrades,\n",
    "            key=lambda x: x[\"price_delta_usd\"],\n",
    "            reverse=True\n",
    "        ),\n",
    "        \"rating_upgrades_all\": sorted(\n",
    "            rating_upgrades,\n",
    "            key=lambda x: x[\"rating_delta\"],\n",
    "            reverse=True\n",
    "        ),\n",
    "    }\n",
    "\n",
    "# --------------------------------\n",
    "# Apply to ALL properties (Spark)\n",
    "# --------------------------------\n",
    "spark_recommendations = (\n",
    "    df_f\n",
    "    # .select(\"host_id\", \"property_id\", *amenity_features)\n",
    "    .select(\"host_id\", \"property_id\", PRICE_COL, \"ratings\", *amenity_features)\n",
    "    .rdd\n",
    "    .map(compute_recommendations_for_row)\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# Schema\n",
    "# --------------------------------\n",
    "recommendation_schema = StructType([\n",
    "    StructField(\"host_id\", StringType(), True),\n",
    "    StructField(\"property_id\", LongType(), True),\n",
    "    StructField(\n",
    "        \"price_upgrades_all\",\n",
    "        ArrayType(StructType([\n",
    "            StructField(\"feature\", StringType()),\n",
    "            StructField(\"price_delta_usd\", DoubleType()),\n",
    "            StructField(\"coef_mean\", DoubleType()),\n",
    "            StructField(\"coef_std\", DoubleType()),\n",
    "            StructField(\"coef_var\", DoubleType()),\n",
    "            StructField(\"ci_low\", DoubleType()),\n",
    "            StructField(\"ci_high\", DoubleType())\n",
    "        ]))\n",
    "    ),\n",
    "    StructField(\n",
    "        \"rating_upgrades_all\",\n",
    "        ArrayType(StructType([\n",
    "            StructField(\"feature\", StringType()),\n",
    "            StructField(\"rating_delta\", DoubleType()),\n",
    "            StructField(\"coef_mean\", DoubleType()),\n",
    "            StructField(\"coef_std\", DoubleType()),\n",
    "            StructField(\"coef_var\", DoubleType()),\n",
    "            StructField(\"ci_low\", DoubleType()),\n",
    "            StructField(\"ci_high\", DoubleType())\n",
    "        ]))\n",
    "    ),\n",
    "])\n",
    "\n",
    "# --------------------------------\n",
    "# Create Spark DataFrame\n",
    "# --------------------------------\n",
    "spark_with_recos = spark.createDataFrame(\n",
    "    spark_recommendations,\n",
    "    schema=recommendation_schema\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# SAVE (PARQUET ONLY)\n",
    "# --------------------------------\n",
    "spark_with_recos.write.mode(\"overwrite\").parquet(\n",
    "    \"dbfs:/airbnb/property_upgrade_recommendations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a44a2ff-4810-47f7-87c7-6f908a5784c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================\n",
    "# DEBUG PRINT: 3 SAMPLE HOSTS\n",
    "# =============================\n",
    "\n",
    "# 1) Pick 3 distinct host_ids (cheap action)\n",
    "sample_host_ids = (\n",
    "    spark_with_recos\n",
    "    .select(\"host_id\")\n",
    "    .where(F.col(\"host_id\").isNotNull())\n",
    "    .distinct()\n",
    "    .limit(3)\n",
    "    .rdd\n",
    "    .map(lambda r: r[\"host_id\"])\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(\"\\n=== DEBUG: SAMPLE HOST UPGRADE RECOMMENDATIONS ===\")\n",
    "\n",
    "# 2) Print recommendations for those hosts only\n",
    "for hid in sample_host_ids:\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"HOST ID: {hid}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    host_rows = (\n",
    "        spark_with_recos\n",
    "        .where(F.col(\"host_id\") == hid)\n",
    "        .limit(3)   # limit properties per host to keep output readable\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    for r in host_rows:\n",
    "        print(f\"\\nProperty ID: {r['property_id']}\")\n",
    "\n",
    "        print(\"  PRICE upgrades (top 3 shown):\")\n",
    "        for u in (r[\"price_upgrades_all\"] or [])[:3]:\n",
    "            print(\n",
    "                f\"   - {u['feature'].replace('a_', '')}: \"\n",
    "                f\"+${u['price_delta_usd']:.2f} per night \"\n",
    "                f\"| std={u['coef_std']:.4f} | var={u['coef_var']:.4f} \"\n",
    "                f\"| CI=[{u['ci_low']:.4f}, {u['ci_high']:.4f}]\"\n",
    "            )\n",
    "\n",
    "        print(\"  RATING upgrades (top 3 shown):\")\n",
    "        for u in (r[\"rating_upgrades_all\"] or [])[:3]:\n",
    "            print(\n",
    "                f\"   - {u['feature'].replace('a_', '')}: \"\n",
    "                f\"+{u['rating_delta']:.3f} rating | std={u['coef_std']:.4f} | var={u['coef_var']:.4f} | CI=[{u['ci_low']:.4f}, {u['ci_high']:.4f}]\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56cc6afd-ff73-4a30-ac81-29db0840ed28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load final conclusions from DBFS\n",
    "final_conclusions = (\n",
    "    spark.read\n",
    "    .parquet(\"dbfs:/airbnb/final_conclusions\")\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# Optional: clean amenity names for plotting\n",
    "final_conclusions[\"amenity_clean\"] = (\n",
    "    final_conclusions[\"amenity\"]\n",
    "    .str.replace(\"a_\", \"\", regex=False)\n",
    "    .str.replace(\"_\", \" \")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac0ebb3b-85aa-4d31-a763-344249692e73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Modern poster theme ---\n",
    "sns.set_theme(\n",
    "    style=\"white\",\n",
    "    font_scale=1.05,\n",
    "    rc={\n",
    "        \"axes.grid\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"axes.spines.top\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Select top + bottom amenities\n",
    "# -----------------------------\n",
    "top_pos = final_conclusions.sort_values(\"coef_log_price\", ascending=False).head(5)\n",
    "top_neg = final_conclusions.sort_values(\"coef_log_price\", ascending=True).head(5)\n",
    "\n",
    "plot_df = pd.concat([top_neg, top_pos]).sort_values(\"coef_log_price\")\n",
    "\n",
    "# -----------------------------\n",
    "# Color by sign\n",
    "# -----------------------------\n",
    "plot_df[\"bar_color\"] = plot_df[\"coef_log_price\"].apply(\n",
    "    lambda x: \"#4C72B0\" if x > 0 else \"#DD8452\"  # blue / orange\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Plot\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=plot_df,\n",
    "    x=\"coef_log_price\",\n",
    "    y=\"amenity_clean\",\n",
    "    palette=plot_df[\"bar_color\"],\n",
    "    alpha=0.85\n",
    ")\n",
    "\n",
    "# --- Subtle vertical grid ---\n",
    "ax.xaxis.grid(True, color=\"#E6E6E6\", linewidth=0.8)\n",
    "ax.yaxis.grid(False)\n",
    "\n",
    "# -----------------------------\n",
    "# Error bars\n",
    "# -----------------------------\n",
    "for i, row in enumerate(plot_df.itertuples()):\n",
    "    plt.errorbar(\n",
    "        x=row.coef_log_price,\n",
    "        y=i,\n",
    "        xerr=row.coef_std_price,\n",
    "        fmt=\"none\",\n",
    "        ecolor=\"#555555\",\n",
    "        capsize=2,\n",
    "        linewidth=0.9,\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Coefficient annotations (anchored to zero)\n",
    "# -----------------------------\n",
    "for i, row in enumerate(plot_df.itertuples()):\n",
    "    coef = row.coef_log_price\n",
    "\n",
    "    if coef > 0:\n",
    "        x_text, ha = -0.004, \"right\"\n",
    "    else:\n",
    "        x_text, ha = 0.004, \"left\"\n",
    "\n",
    "    plt.text(\n",
    "        x_text,\n",
    "        i,\n",
    "        f\"{coef:+.3f}\",\n",
    "        va=\"center\",\n",
    "        ha=ha,\n",
    "        fontsize=9.5,\n",
    "        fontweight=\"semibold\",\n",
    "        color=\"#2F2F2F\"\n",
    "    )\n",
    "\n",
    "# --- Zero line ---\n",
    "plt.axvline(0, color=\"#444444\", linewidth=1.2)\n",
    "\n",
    "# -----------------------------\n",
    "# Labels & title\n",
    "# -----------------------------\n",
    "plt.xlabel(\"Effect on price\", fontsize=11)\n",
    "plt.ylabel(\"\")\n",
    "plt.title(\n",
    "    \"Top Positive and Negative Amenity Effects\",\n",
    "    fontsize=13,\n",
    "    weight=\"semibold\",\n",
    "    pad=8\n",
    ")\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecddd3c5-68aa-442f-b10f-25860320a945",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- SAME poster theme ---\n",
    "sns.set_theme(\n",
    "    style=\"white\",\n",
    "    font_scale=1.05,\n",
    "    rc={\n",
    "        \"axes.grid\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"axes.spines.top\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Select only significant amenities\n",
    "# -----------------------------\n",
    "tradeoff_df = final_conclusions.copy()\n",
    "\n",
    "# Define overall impact\n",
    "tradeoff_df[\"impact\"] = (\n",
    "    tradeoff_df[\"coef_log_price\"].abs() +\n",
    "    tradeoff_df[\"coef_rating\"].abs()\n",
    ")\n",
    "\n",
    "TOP_K = 12  # adjust if needed (6–10 works best for posters)\n",
    "\n",
    "plot_df = (\n",
    "    tradeoff_df\n",
    "    .sort_values(\"impact\", ascending=False)\n",
    "    .head(TOP_K)\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Plot\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(6.5, 4.5))\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "    data=plot_df,\n",
    "    x=\"coef_log_price\",\n",
    "    y=\"coef_rating\",\n",
    "    s=90,\n",
    "    color=\"#4C72B0\",\n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "# --- Zero reference lines ---\n",
    "plt.axvline(0, color=\"#444444\", linewidth=1.2)\n",
    "plt.axhline(0, color=\"#444444\", linewidth=1.2)\n",
    "\n",
    "# --- Subtle grid ---\n",
    "ax.xaxis.grid(True, color=\"#E6E6E6\", linewidth=0.8)\n",
    "ax.yaxis.grid(True, color=\"#E6E6E6\", linewidth=0.8)\n",
    "\n",
    "# -----------------------------\n",
    "# Labels for ALL shown points\n",
    "# -----------------------------\n",
    "for _, row in plot_df.iterrows():\n",
    "    plt.text(\n",
    "        row[\"coef_log_price\"],\n",
    "        row[\"coef_rating\"] - 0.0002,  # slight vertical offset downward\n",
    "        row[\"amenity_clean\"],\n",
    "        fontsize=9.5,\n",
    "        fontweight=\"semibold\",\n",
    "        ha=\"center\",\n",
    "        va=\"top\",\n",
    "        alpha=0.95\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Labels & title\n",
    "# -----------------------------\n",
    "plt.xlabel(\"Effect on price\", fontsize=11)\n",
    "plt.ylabel(\"Effect on rating\", fontsize=11)\n",
    "\n",
    "plt.title(\n",
    "    \"Price vs Rating Tradeoff\",\n",
    "    fontsize=13,\n",
    "    weight=\"semibold\",\n",
    "    pad=8\n",
    ")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c177a0f2-685c-4c18-8e5f-284bdb41deaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e60da3-ddf4-4c20-94e4-6cf6a35fbbde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Orders = number of rows\n",
    "orders = airbnb.count()\n",
    "\n",
    "# Hosts\n",
    "hosts = (\n",
    "    airbnb\n",
    "    .select(F.col(\"seller_info\"))\n",
    "    .where(F.col(\"seller_info\").isNotNull())\n",
    "    .select(F.from_json(\"seller_info\", \"struct<seller_id:string>\").alias(\"s\"))\n",
    "    .select(\"s.seller_id\")\n",
    "    .distinct()\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Properties (prefer property_id / listing_id if exists)\n",
    "property_id_col = next(\n",
    "    (c for c in [\"property_id\", \"listing_id\", \"id\"] if c in airbnb.columns),\n",
    "    None\n",
    ")\n",
    "\n",
    "properties = (\n",
    "    airbnb\n",
    "    .select(property_id_col)\n",
    "    .distinct()\n",
    "    .count()\n",
    ")\n",
    "\n",
    "amenities_schema = \"array<struct<group_name:string, items:array<struct<name:string,value:string>>>>\"\n",
    "\n",
    "amenities_count = (\n",
    "    airbnb\n",
    "    .withColumn(\"amenities_parsed\", F.from_json(\"amenities\", amenities_schema))\n",
    "    .withColumn(\"amenity\", F.explode(F.expr(\"flatten(transform(amenities_parsed, g -> g.items))\")))\n",
    "    .select(F.lower(\"amenity.name\").alias(\"amenity_name\"))\n",
    "    .distinct()\n",
    "    .count()\n",
    ")\n",
    "\n",
    "# Parse reviews array\n",
    "reviews_df = (\n",
    "    airbnb\n",
    "    .withColumn(\"reviews_parsed\", F.from_json(\"reviews\", \"array<string>\"))\n",
    "    .withColumn(\"num_reviews\", F.size(\"reviews_parsed\"))\n",
    ")\n",
    "\n",
    "# Total reviews\n",
    "total_reviews = (\n",
    "    reviews_df\n",
    "    .select(F.sum(\"num_reviews\").alias(\"total\"))\n",
    "    .collect()[0][\"total\"]\n",
    ")\n",
    "\n",
    "# Reviews per property (average)\n",
    "avg_reviews_per_property = (\n",
    "    reviews_df\n",
    "    .select(F.avg(\"num_reviews\").alias(\"avg\"))\n",
    "    .collect()[0][\"avg\"]\n",
    ")\n",
    "\n",
    "print(\"Airbnb Data Statistics\")\n",
    "print(\"----------------------\")\n",
    "print(f\"Orders: {orders:,}\")\n",
    "print(f\"Hosts: {hosts:,}\")\n",
    "print(f\"Properties: {properties:,}\")\n",
    "print(f\"Amenities: {amenities_count:,}\")\n",
    "print(f\"Reviews: {total_reviews:,}\")\n",
    "print(f\"Reviews per Property (avg): {avg_reviews_per_property:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72003a91-3c4d-4587-9f8e-0b74bc5ecb50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sns.set_theme(\n",
    "    style=\"white\",\n",
    "    font_scale=1.05,\n",
    "    rc={\n",
    "        \"axes.grid\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"axes.spines.top\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7687935e-4ef5-4431-97b7-c8f0b48eaabf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratings_df = (\n",
    "    airbnb\n",
    "    .select(F.col(\"ratings\").cast(\"double\").alias(\"rating\"))\n",
    "    .where(F.col(\"rating\").isNotNull())\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(4.5,3))\n",
    "\n",
    "sns.histplot(\n",
    "    ratings_df[\"rating\"],\n",
    "    bins=20,\n",
    "    color=\"#4C72B0\",\n",
    "    edgecolor=\"white\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Number of Listings\")\n",
    "plt.title(\"Rating Distribution\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b22e594-b29c-469f-8bd3-90950cbe3405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare price data\n",
    "price_df = (\n",
    "    df\n",
    "    .select(F.col(\"price_per_night\").cast(\"double\").alias(\"price\"))\n",
    "    .where((F.col(\"price\") > 0) & (F.col(\"price\") < 500))  # optional cap for clarity\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# Define price bins\n",
    "bins = [0, 50, 100, 150, 200, 300, 500]\n",
    "labels = [\"0–50\", \"50–100\", \"100–150\", \"150–200\", \"200–300\", \"300+\"]\n",
    "\n",
    "price_df[\"price_range\"] = pd.cut(\n",
    "    price_df[\"price\"],\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    right=False\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "\n",
    "ax = sns.countplot(\n",
    "    data=price_df,\n",
    "    x=\"price_range\",\n",
    "    color=\"#4C72B0\"\n",
    ")\n",
    "\n",
    "ax.yaxis.grid(True, color=\"#E6E6E6\", linewidth=0.8)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "plt.xlabel(\"Price per Night ($)\")\n",
    "plt.ylabel(\"Number of Listings\")\n",
    "plt.title(\"Price Distribution by Range\")\n",
    "\n",
    "# \uD83D\uDD39 Rotate price range labels\n",
    "plt.xticks(rotation=90, ha=\"right\")\n",
    "\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69196237-c406-4c64-89ed-280a412a266d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "amenities_schema = \"\"\"\n",
    "array<struct<\n",
    "    group_name:string,\n",
    "    items:array<struct<name:string,value:string>>\n",
    ">>\n",
    "\"\"\"\n",
    "\n",
    "top_amenities = (\n",
    "    airbnb\n",
    "    .withColumn(\"amenities_parsed\", F.from_json(\"amenities\", amenities_schema))\n",
    "    .withColumn(\"amenity\", F.explode(F.expr(\"flatten(transform(amenities_parsed, g -> g.items))\")))\n",
    "    .select(F.lower(\"amenity.name\").alias(\"amenity\"))\n",
    "    .groupBy(\"amenity\")\n",
    "    .count()\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "    .limit(10)\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(5,3.5))\n",
    "\n",
    "sns.barplot(\n",
    "    data=top_amenities,\n",
    "    x=\"count\",\n",
    "    y=\"amenity\",\n",
    "    color=\"#4C72B0\",\n",
    "    alpha=0.85\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of Listings\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title(\"Top 10 Most Common Amenities\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0cde40d-3c81-43d4-b912-e83bed6e6b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Poster theme\n",
    "# -----------------------------\n",
    "sns.set_theme(\n",
    "    style=\"white\",\n",
    "    font_scale=1.05,\n",
    "    rc={\n",
    "        \"axes.grid\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"axes.spines.top\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare data\n",
    "# -----------------------------\n",
    "eda_df = (\n",
    "    df\n",
    "    .select(\n",
    "        F.col(\"price_per_night\").cast(\"double\").alias(\"price\"),\n",
    "        F.col(\"ratings\").cast(\"double\").alias(\"rating\")\n",
    "    )\n",
    "    .where(\n",
    "        (F.col(\"price\") > 0) &\n",
    "        (F.col(\"price\") <= 500) &\n",
    "        (F.col(\"rating\").between(1, 5))\n",
    "    )\n",
    "    .toPandas()\n",
    ")\n",
    "\n",
    "# Price ranges\n",
    "bins = [0, 50, 100, 150, 200, 300, 500, 1000]\n",
    "labels = [\"0–50\", \"50–100\", \"100–150\", \"150–200\", \"200–300\", \"300-500\", \"500+\"]\n",
    "\n",
    "eda_df[\"price_range\"] = pd.cut(\n",
    "    eda_df[\"price\"],\n",
    "    bins=bins,\n",
    "    labels=labels,\n",
    "    right=False\n",
    ")\n",
    "\n",
    "# Aggregate: mean + CI\n",
    "summary = (\n",
    "    eda_df\n",
    "    .groupby(\"price_range\")\n",
    "    .agg(\n",
    "        mean_rating=(\"rating\", \"mean\"),\n",
    "        std_rating=(\"rating\", \"std\"),\n",
    "        n=(\"rating\", \"count\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "summary[\"ci\"] = 1.96 * summary[\"std_rating\"] / np.sqrt(summary[\"n\"])\n",
    "\n",
    "# -----------------------------\n",
    "# Plot\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(5.2, 3.4))\n",
    "\n",
    "# Points + confidence intervals\n",
    "plt.errorbar(\n",
    "    x=summary[\"price_range\"],\n",
    "    y=summary[\"mean_rating\"],\n",
    "    yerr=summary[\"ci\"],\n",
    "    fmt=\"o\",\n",
    "    color=\"#4C72B0\",\n",
    "    ecolor=\"#4C72B0\",\n",
    "    elinewidth=1.2,\n",
    "    capsize=3,\n",
    "    markersize=6\n",
    ")\n",
    "\n",
    "# Subtle connecting trend line\n",
    "plt.plot(\n",
    "    summary[\"price_range\"],\n",
    "    summary[\"mean_rating\"],\n",
    "    color=\"#4C72B0\",\n",
    "    linewidth=1,\n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Subtle horizontal grid\n",
    "plt.gca().yaxis.grid(True, color=\"#E6E6E6\", linewidth=0.8)\n",
    "\n",
    "# Annotate only min & max price ranges\n",
    "for i in [0, len(summary) - 1]:\n",
    "    plt.text(\n",
    "        i,\n",
    "        summary.loc[i, \"mean_rating\"] + 0.01,\n",
    "        f'{summary.loc[i, \"mean_rating\"]:.2f}',\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "        fontweight=\"semibold\"\n",
    "    )\n",
    "\n",
    "# Labels & title\n",
    "plt.xlabel(\"Price per Night ($)\")\n",
    "plt.ylabel(\"Average Rating\")\n",
    "plt.title(\n",
    "    \"Average Rating by Price Range\",\n",
    "    fontsize=13,\n",
    "    weight=\"semibold\",\n",
    "    pad=8\n",
    ")\n",
    "\n",
    "plt.ylim(4.5, 5.05)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7263496967750255,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Linear_Regression + EDA graps",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}